{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"card2code_5-17.ipynb","provenance":[],"collapsed_sections":["ySNzNHIl5knR","iu-lMrLSB-VX"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCFbP1kfDrn-","executionInfo":{"status":"ok","timestamp":1621302932411,"user_tz":240,"elapsed":24474,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}},"outputId":"2ab7e925-f6b4-40e9-a095-c3848780c2a3"},"source":["from google.colab import drive \n","drive.mount(\"/content/drive\")\n","project_dir = \"/content/drive/MyDrive/6.806 6.864 Final Project\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygPk97PVEARv","executionInfo":{"status":"ok","timestamp":1621302941385,"user_tz":240,"elapsed":6307,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}},"outputId":"5ed7c0d6-e199-479a-aaf2-b15c1db0af00"},"source":["%%bash\n","pip install tokenizers\n","pip install sacrebleu"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting tokenizers\n","  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","Installing collected packages: tokenizers\n","Successfully installed tokenizers-0.10.2\n","Collecting sacrebleu\n","  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n","Collecting portalocker==2.0.0\n","  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n","Installing collected packages: portalocker, sacrebleu\n","Successfully installed portalocker-2.0.0 sacrebleu-1.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WkERchwbSbD-","executionInfo":{"status":"ok","timestamp":1621302946850,"user_tz":240,"elapsed":3947,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["import math\n","from tqdm import tqdm\n","\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils import data\n","from torch import cuda\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","from tokenizers import Tokenizer\n","from tokenizers.trainers import BpeTrainer\n","from tokenizers.models import BPE\n","from tokenizers.pre_tokenizers import Whitespace\n","\n","import os\n","import json\n","\n","import sacrebleu\n","\n","device = \"cuda\" if cuda.is_available() else \"cpu\"\n","\n","train_input_path = os.path.join(project_dir, \"hearthstone\", \"train_hs.in\")\n","train_target_path = os.path.join(project_dir, \"hearthstone\", \"train_hs.out\")\n","test_input_path = os.path.join(project_dir, \"hearthstone\", \"test_hs.in\")\n","test_target_path = os.path.join(project_dir, \"hearthstone\", \"test_hs.out\")"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6I-qxVyrLGNi"},"source":["# Parsing\n","In this first section, we parse the input and outputs texts. This consists of tokenizing the texts and converting tokens into IDs"]},{"cell_type":"code","metadata":{"id":"4oKrzajuUI5R"},"source":["PAD_ID = 0\n","PAD_TOKEN = \"PAD_INDEX\"\n","SOS_ID = 2\n","SOS_TOKEN = \"SOS_INDEX\"\n","EOS_ID = 3\n","EOS_TOKEN = \"EOS_INDEX\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EVl2TQ4zeKn8","executionInfo":{"status":"ok","timestamp":1621302947973,"user_tz":240,"elapsed":943,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["def read_file(path):\n","    \"\"\"Reads file and returns a list of each line\"\"\"\n","    with open(path) as f:\n","        return f.readlines()\n","\n","def tokenize_input_corpus(inputs, mode, token2id=None):\n","    \"\"\"Tokenize corpus (list of texts) and returns IDs of each text\"\"\"\n","    line_tokens = [line.split() for line in inputs]\n","    max_length = max([len(tokens) for tokens in line_tokens])\n","    if not token2id:\n","        special_tokens = [\n","            \"PAD_INDEX\", \"UNK\", \"SOS_INDEX\", \"EOS_INDEX\", \"NAME_END\", \"ATK_END\", \"DEF_END\",\n","            \"COST_END\", \"DUR_END\", \"TYPE_END\", \"PLAYER_CLS_END\", \"RACE_END\", \"RARITY_END\"\n","        ]\n","        token2id = { token: idx for (idx, token) in enumerate(special_tokens) }\n","    tokenized_corpus = []\n","    line_lens = []\n","    for tokens in line_tokens:\n","        tokens = line.split()\n","        word_ids, token2id, ids_len = process_line(tokens, token2id, mode, \n","                                                     max_length)\n","        tokenized_corpus.append(word_ids)\n","        line_lens.append(ids_len)\n","    return tokenized_corpus, vocab_dict, line_lens\n","        \n","def tokens_to_ids(tokens, token2id, mode, max_length):\n","    \"\"\"Convert list of tokens to IDs\"\"\"\n","    token_ids = []\n","    for token in tokens:\n","        if token in token2id:\n","            token_ids.append(token2id[token])\n","        elif mode == \"train\":\n","            token_ids.append(len(token2id))\n","            token2id[token] = len(token2id)\n","        else:\n","            token_ids.append(token2id[\"UNK\"])\n","    ids_len = len(token_ids) + 2 \n","    token_ids = [token2id[\"SOS_INDEX\"]] + token_ids + \\\n","                   [token2id[\"EOS_INDEX\"]] + \\\n","                   [token2id[\"PAD_INDEX\"]]*(max_length - ids_len)\n","    return token_ids, token2id, ids_len\n","\n","def batch_tokens_to_ids(line_tokens, token2id, mode, max_length=None):\n","    \"\"\"Convert tokens to IDs for a batch and returns long tensor for whole batch of IDs\"\"\"\n","    if max_length is None:\n","        max_length = max([len(line) for line in line_tokens])\n","    line_ids = []\n","    line_lens = []\n","    for tokens in line_tokens:\n","        ids, token2id, line_len = tokens_to_ids(tokens, token2id, mode, max_length=max_length)\n","        line_ids.append(ids)\n","        line_lens.append(line_len)\n","    return torch.LongTensor(line_ids).to(device), token2id, torch.LongTensor(line_lens).to(device)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"iByt3yvPHBtY","executionInfo":{"status":"ok","timestamp":1621302948790,"user_tz":240,"elapsed":586,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["def save_mapping(mapping, path):\n","    with open(path, 'w') as f:\n","        json.dump(mapping, f)\n","        \n","def load_mapping(path):\n","    with open(path) as f:\n","        return json.load(f)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TIISVjk2LR-o"},"source":["## Byte Pair Encoding Tokenizer\n","We experiment with tokenizing using Huggingface's byte pair encoding tokenizer"]},{"cell_type":"code","metadata":{"id":"6NJQbNwlDEuF","executionInfo":{"status":"ok","timestamp":1621302949515,"user_tz":240,"elapsed":730,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["def get_trained_tokenizer(train_paths):\n","    \"\"\"Use huggingface tokenizer and train on corpus\"\"\"\n","    tokenizer = Tokenizer(BPE())\n","    tokenizer.pre_tokenizer = Whitespace()\n","    trainer = BpeTrainer()\n","    tokenizer.train(files=train_paths)\n","    return tokenizer"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"7SQeVLlfDEuG"},"source":["# Train tokenizer for inputs and targets\n","input_special_tokens = [\n","    \"PAD_INDEX\", \"UNK\", \"SOS_INDEX\", \"EOS_INDEX\", \"NAME_END\", \"ATK_END\", \"DEF_END\",\n","    \"COST_END\", \"DUR_END\", \"TYPE_END\", \"PLAYER_CLS_END\", \"RACE_END\", \"RARITY_END\"\n","]\n","train_raw_inputs = read_file(train_input_path)\n","input_tokenizer = get_trained_tokenizer([train_input_path])\n","\n","target_special_tokens = [\"PAD_INDEX\", \"UNK\", \"SOS_INDEX\", \"EOS_INDEX\"]\n","train_raw_targets = read_file(train_target_path)\n","target_tokenizer = get_trained_tokenizer([train_target_path])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NY-5BkfUDEuG"},"source":["# Get tokens for inputs and targets\n","input_line_tokens = [e.tokens for e in input_tokenizer.encode_batch(train_raw_inputs)]\n","target_line_tokens = [e.tokens for e in target_tokenizer.encode_batch(train_raw_targets)]\n","\n","# Create mapping of tokens to IDs for inputs and targets\n","input_tokens = set([t for line in input_line_tokens for t in line]) - set(input_special_tokens)\n","all_input_tokens = input_special_tokens + sorted(list(input_tokens))\n","input_token2id = { token: id for (id, token) in enumerate(all_input_tokens) }\n","input_id2token = { id: token for (token, id) in input_token2id.items() }\n","\n","target_tokens = set([t for line in target_line_tokens for t in line]) - set(target_special_tokens)\n","all_target_tokens = target_special_tokens + sorted(list(target_tokens))\n","target_token2id = { token: id for (id, token) in enumerate(all_target_tokens) }\n","target_id2token = { id: token for (token, id) in target_token2id.items() }\n","\n","# Get longest length of tokens (+2 for start and end tokens)\n","input_max_seq_len = max([len(line) for line in input_line_tokens]) + 2\n","target_max_seq_len = max([len(line) for line in target_line_tokens]) + 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3Fha8sKHE2O"},"source":["# Set to True if you want to save these mappings\n","save_mappings = False\n","if save_mappings:\n","    save_mapping(input_token2id, os.path.join(project_dir, \"input_token2id.json\"))\n","    save_mapping(target_token2id, os.path.join(project_dir, \"target_token2id.json\"))\n","\n","# Set to True if you want to load previously used mappings\n","load_mappings = False\n","if load_mappings:\n","    input_token2id = load_mapping(os.path.join(project_dir, \"input_token2id.json\"))\n","    target_token2id = load_mapping(os.path.join(project_dir, \"target_token2id.json\"))\n","    input_id2token = { id: token for (token, id) in input_token2id.items() }\n","    target_id2token = { id: token for (token, id) in target_token2id.items() }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wKxhCHBkLfdx"},"source":["# Organizing Data\n","We will organize the input and output tokens above into a dataset object for easier use during training. This dataset will convert tokens into padded ID sequences."]},{"cell_type":"code","metadata":{"id":"M1zCFeEoDEuG","executionInfo":{"status":"ok","timestamp":1621302952601,"user_tz":240,"elapsed":538,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["class SimpleHearthstoneDataset(data.Dataset):\n","    \"\"\"Simple dataset with input and output tokens as IDs\"\"\"\n","    def __init__(self, input_line_tokens, target_line_tokens, input_token2id, target_token2id, mode, input_max_seq_len=None, target_max_seq_len=None):\n","        self.input_line_ids, _, self.input_line_lens = batch_tokens_to_ids(input_line_tokens, input_token2id, mode, max_length=input_max_seq_len)\n","        self.target_line_ids, _, self.target_line_lens = batch_tokens_to_ids(target_line_tokens, target_token2id, mode, max_length=target_max_seq_len)\n","        \n","    def __len__(self):\n","        return len(self.input_line_ids)\n","    \n","    def __getitem__(self, idx):\n","        return self.input_line_ids[idx], self.target_line_ids[idx], self.input_line_lens[idx], self.target_line_lens[idx]"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"z3ktG3mdDEuH"},"source":["# Split validation and training data\n","validation_ratio = 0.1\n","train_size = int((1 - validation_ratio) * len(input_line_tokens))\n","train_input_line_tokens = input_line_tokens[: train_size]\n","train_target_line_tokens = target_line_tokens[: train_size]\n","validation_input_line_tokens = input_line_tokens[train_size:]\n","validation_target_line_tokens = target_line_tokens[train_size:]\n","\n","# Create datasets for validation and training\n","simple_train_dataset = SimpleHearthstoneDataset(train_input_line_tokens, train_target_line_tokens, input_token2id, target_token2id, \"train\", input_max_seq_len=input_max_seq_len, target_max_seq_len=target_max_seq_len)\n","simple_validation_dataset = SimpleHearthstoneDataset(validation_input_line_tokens, validation_target_line_tokens, input_token2id, target_token2id, \"train\", input_max_seq_len=input_max_seq_len, target_max_seq_len=target_max_seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"prfDXGv6RrU2"},"source":["# Simple Seq2Seq Model\n","We start by experimenting with a simple seq2seq model w/attention to get benchmark performance\n","1. Tokenize each word without separating into fields\n","2. Encode input sequences using bi-RNN\n","3. Use last hidden layer and outputs of encoder in decoder to generate output tokens"]},{"cell_type":"code","metadata":{"id":"tyVn_NORSXTN","executionInfo":{"status":"ok","timestamp":1621302955951,"user_tz":240,"elapsed":463,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["class SimpleHearthstoneEncoder(nn.Module):\n","    \"\"\"Simple encoder for hearthstone tokens\"\"\"\n","    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers=3, dropout=0.1):\n","        super(SimpleHearthstoneEncoder, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_size)\n","        self.rnn = nn.GRU(\n","            input_size=embedding_size,\n","            hidden_size=hidden_size,\n","            num_layers=3,\n","            dropout=dropout,\n","            batch_first=True,\n","            bidirectional=True\n","        )\n","\n","    def forward(self, inputs, lengths, max_seq_length=None):\n","        \"\"\"\n","        :param inputs: 3d tensor of shape (batch_size, max_seq_length, embed_size)\n","        :param lengths: 1d tensor of shape (batch_size,)\n","\n","        :return: (outputs, finals) where outputs is 3d tensor of shape (batch_size, max_seq_length, hidden_size)\n","                and finals is 3d tensor of shape (num_layers, batch_size, 2*hidden_size)\n","        \"\"\"\n","        if max_seq_length is None:\n","            max_seq_length = inputs.size(1)\n","            \n","        embedded_inputs = self.embedding(inputs)\n","        packed = pack_padded_sequence(embedded_inputs, lengths.cpu(), batch_first=True, enforce_sorted=False)\n","        outputs, hidden = self.rnn(packed)\n","        outputs, _ = pad_packed_sequence(outputs, batch_first=True, total_length=max_seq_length)\n","\n","        forward_hidden = hidden[::2]\n","        backward_hidden = hidden[1::2]\n","        hidden = torch.cat([forward_hidden, backward_hidden], dim=2)\n","\n","        return outputs, hidden"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"gpZl9LH8Xvte","executionInfo":{"status":"ok","timestamp":1621302956772,"user_tz":240,"elapsed":992,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["# Decoder with attention architecture based on\n","# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","# http://www.davidsbatista.net/blog/2020/01/25/Attention-seq2seq/\n","# https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html\n","\n","class SimpleHearthstoneDecoder(nn.Module):\n","    \"\"\"Simple decoder with attention for hearthstone tokens to python code\"\"\"\n","    def __init__(self, vocab_size, embedding_size, hidden_size, enc_hidden_size, enc_max_seq_length, rnn_num_layers=3, dropout=0.1):\n","        # TODO: add dropout?\n","        super(SimpleHearthstoneDecoder, self).__init__()\n","        self.bridge = nn.Linear(enc_hidden_size, hidden_size)\n","        self.embedding = nn.Embedding(vocab_size, embedding_size)\n","        \n","        self.dropout = nn.Dropout(p=dropout)\n","        self.attention = nn.Linear(embedding_size + hidden_size, enc_max_seq_length)\n","        self.combine_attention = nn.Linear(enc_hidden_size + embedding_size, hidden_size)\n","        \n","        self.rnn = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, batch_first=True, num_layers=rnn_num_layers)\n","\n","    def forward_step(self, prev_embed, hidden, encoder_outputs):\n","        \"\"\"\n","        :param prev_embed: 3d tensor of shape (batch_size, 1, embed_size) containing word embeddings\n","                from previous time step\n","        :param hidden: 3d tensor of shape (num_layers, batch_size, hidden_size) representing current decoder hidden state\n","        :param encoder_outputs: 3d tensor of shape (batch_size, max_seq_length, enc_hidden_size) representing output layers of encoder\n","                for all time steps\n","\n","        :return: [pre_output, hidden] of current time step\n","        \"\"\"\n","        # Use previous embedding and last hidden layer to compute attention weights\n","        concat_prev_embed = torch.cat((prev_embed.squeeze(1), hidden[-1]), dim=-1)\n","        attention_raw = self.attention(concat_prev_embed)\n","        attention_weights = F.softmax(attention_raw)\n","        # Apply attention weights to encoder outputs\n","        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n","\n","        # Combine context vector with previous embedding\n","        prev_embed_with_context = torch.cat((prev_embed, context), dim=-1).squeeze(1)\n","        prev_embed_with_context = self.combine_attention(prev_embed_with_context)\n","        prev_embed_with_context = F.relu(prev_embed_with_context).unsqueeze(1)\n","\n","        return self.rnn(prev_embed_with_context, hidden)\n","\n","    def forward(self, inputs, encoder_outputs, encoder_hidden, hidden=None, max_output_len=None):\n","        \"\"\"\n","        :param inputs: 3d tensor of shape (batch_size, max_seq_length) with target sentences\n","        :param encoder_outputs: 3d tensor of shape (batch_size, max_seq_length, enc_hidden_size) with output layers from encoder\n","        :param encoder_hidden: 3d tensor of shape (num_enc_layers, batch_size, hidden_size) with final encoder hidden state\n","        :param hidden: 3d tensor of shape (1, batch_size, hidden_size) with hidden state from previous time step\n","        :param max_output_len: int maximum length of output sequence\n","        \"\"\"\n","        # Initialize values if not given\n","        if max_output_len is None:\n","            max_output_len = inputs.size(1)\n","        if hidden is None:\n","            hidden = self.init_hidden(encoder_hidden)\n","            \n","        embedded = self.embedding(inputs)\n","        dropped_embedded = self.dropout(embedded)\n","\n","        # Generate output and hidden for each word\n","        pre_output_vectors = []\n","        for i in range(max_output_len):\n","            prev_embed = dropped_embedded[:, i].unsqueeze(1)\n","            pre_output, hidden = self.forward_step(prev_embed, hidden, encoder_outputs)\n","            pre_output_vectors.append(pre_output)\n","\n","        outputs = torch.cat(pre_output_vectors, dim=1)\n","        return outputs, hidden\n","\n","    def init_hidden(self, encoder_hidden):\n","        \"\"\"\n","        :param encoder_hidden: 3d tensor of shape (num_enc_layers, batch_size, hidden_size) with final encoder hidden state\n","        \"\"\"\n","        return torch.tanh(self.bridge(encoder_hidden))"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"QRVbd0FevFCM","executionInfo":{"status":"ok","timestamp":1621302957977,"user_tz":240,"elapsed":811,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["class SimpleHearthstoneEncoderDecoder(nn.Module):\n","    def __init__(self, encoder, decoder, generator):\n","        \"\"\"\n","        Inputs:\n","          - `encoder`: an `Encoder` object.\n","          - `decoder`: a `Decoder` object.\n","          - `generator`: a `Generator` object. Essentially a linear mapping. See\n","              the next code cell.\n","        \"\"\"\n","        super(SimpleHearthstoneEncoderDecoder, self).__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.generator = generator\n","\n","    def forward(self, src_ids, trg_ids, src_lengths):\n","        \"\"\"Take in and process masked source and target sequences.\n","\n","        Inputs:\n","          `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n","            a batch of source sentences of word ids.\n","          `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n","            a batch of target sentences of word ids.\n","          `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n","            sequence length of `src_ids`.\n","\n","        Returns the decoder outputs, see the above cell.\n","        \"\"\"\n","        encoder_outputs, encoder_hidden = self.encode(src_ids, src_lengths)\n","        return self.decode(trg_ids[:, :-1], encoder_outputs, encoder_hidden)\n","\n","    def encode(self, src_ids, src_lengths):\n","        return self.encoder(src_ids, src_lengths)\n","\n","    def decode(self, trg_ids, encoder_outputs, encoder_hidden, decoder_hidden=None):\n","        return self.decoder(trg_ids, encoder_outputs, encoder_hidden, hidden=decoder_hidden)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"POvpT2oCDEuJ","executionInfo":{"status":"ok","timestamp":1621302958254,"user_tz":240,"elapsed":328,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["class Generator(nn.Module):\n","    \"\"\"Define standard linear + softmax generation step.\"\"\"\n","    def __init__(self, hidden_size, vocab_size):\n","        super(Generator, self).__init__()\n","        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n","\n","    def forward(self, x):\n","        return F.log_softmax(self.proj(x), dim=-1)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gnmaHFi3MKek"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"5hwdU0D2DEuJ","executionInfo":{"status":"ok","timestamp":1621302959002,"user_tz":240,"elapsed":300,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["class SimpleLossCompute:\n","    \"\"\"A simple loss compute and train function.\"\"\"\n","\n","    def __init__(self, generator, criterion, opt=None):\n","        self.generator = generator\n","        self.criterion = criterion\n","        self.opt = opt\n","\n","    def __call__(self, x, y, norm):\n","        x = self.generator(x)\n","        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n","                                y.contiguous().view(-1))\n","        loss = loss / norm\n","\n","        if self.opt is not None:    # training mode\n","            loss.backward()            \n","            self.opt.step()\n","            self.opt.zero_grad()\n","\n","        return loss.data.item() * norm"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"W6RLVmxADEuK","executionInfo":{"status":"ok","timestamp":1621302960629,"user_tz":240,"elapsed":453,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["def run_epoch(data_loader, model, loss_compute):\n","    \"\"\"Standard Training and Logging Function\"\"\"\n","    total_tokens = 0\n","    total_loss = 0\n","\n","    for i, (src_ids_BxT, trg_ids_BxL, src_lengths_B, trg_lengths_B) in enumerate(tqdm(data_loader, position=0, leave=True)):\n","        # We define some notations here to help you understand the loaded tensor\n","        # shapes:\n","        #     `B`: batch size\n","        #     `T`: max sequence length of source sentences\n","        #     `L`: max sequence length of target sentences; due to our preprocessing\n","        #        in the beginning, `L` == `T` == 50\n","        # An example of `src_ids_BxT` (when B = 2):\n","        #     [[2, 4, 6, 7, ..., 4, 3, 0, 0, 0],\n","        #    [2, 8, 6, 5, ..., 9, 5, 4, 3, 0]]\n","        # The corresponding `src_lengths_B` would be [47, 49].\n","\n","        src_ids_BxT = src_ids_BxT.to(device)\n","        src_lengths_B = src_lengths_B.to(device)\n","        trg_ids_BxL = trg_ids_BxL.to(device)\n","\n","        del trg_lengths_B     # unused\n","\n","        output, _ = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n","\n","        loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:],\n","                            norm=src_ids_BxT.size(0))\n","        total_loss += loss\n","        total_tokens += (trg_ids_BxL[:, 1:] != PAD_ID).data.sum().item()\n","\n","    print(f\"Total loss: {math.exp(total_loss / float(total_tokens))}\")\n","\n","    return math.exp(total_loss / float(total_tokens))\n","\n","def train(model, train_data_loader, val_data_loader, num_epochs, learning_rate):\n","    # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n","    # computing the loss.\n","    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_ID)\n","    optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Keep track of dev ppl for each epoch.\n","    dev_ppls = []\n","\n","    for epoch in range(num_epochs):\n","        print(\"Epoch\", epoch)\n","\n","        model.train()\n","        train_ppl = run_epoch(data_loader=train_data_loader, model=model,\n","                                loss_compute=SimpleLossCompute(model.generator,\n","                                                             criterion, optim))\n","\n","        model.eval()\n","        with torch.no_grad():        \n","            dev_ppl = run_epoch(data_loader=val_data_loader, model=model,\n","                                loss_compute=SimpleLossCompute(model.generator,\n","                                                             criterion, None))\n","            print(\"Validation perplexity: %f\" % dev_ppl)\n","            dev_ppls.append(dev_ppl)\n","        \n","    return dev_ppls"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"8u8_T2-uDEuL"},"source":["# Set params\n","input_embedding_size = 256\n","input_hidden_size = 256\n","target_embedding_size = 256\n","target_hidden_size = 256\n","batch_size = 8\n","\n","# Create data objects\n","simple_train_dataloader = data.DataLoader(simple_train_dataset, batch_size=batch_size, shuffle=True)\n","simple_validation_dataloader = data.DataLoader(simple_validation_dataset, batch_size=batch_size, shuffle=True)\n","input_vocab_size = len(input_token2id)\n","target_vocab_size = len(target_token2id)\n","\n","# Create models\n","simple_encoder = SimpleHearthstoneEncoder(input_vocab_size, input_embedding_size, input_hidden_size).to(device)\n","simple_decoder = SimpleHearthstoneDecoder(target_vocab_size, target_embedding_size, target_hidden_size, 2 * input_hidden_size, input_max_seq_len).to(device)\n","simple_generator = Generator(target_hidden_size, target_vocab_size)\n","simple_encoder_decoder = SimpleHearthstoneEncoderDecoder(simple_encoder, simple_decoder, simple_generator).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3qENSRXfDEuL","outputId":"a945b334-9930-43f7-e517-6f8baf33abae"},"source":["# Train model\n","epochs = 20\n","lr = 1e-3\n","\n","train(simple_encoder_decoder, simple_train_dataloader, simple_validation_dataloader, epochs, lr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","100%|██████████| 60/60 [00:32<00:00,  1.87it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 96.70452770506758\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.76it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 66.02021267971398\n","Validation perplexity: 66.020213\n","Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.88it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 48.98931493740771\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.62it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 51.30249561738432\n","Validation perplexity: 51.302496\n","Epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.87it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 38.906787834542925\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.58it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 45.68239112190183\n","Validation perplexity: 45.682391\n","Epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.88it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 33.67026832979503\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.55it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 41.78727966139444\n","Validation perplexity: 41.787280\n","Epoch 4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.88it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.84it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 29.324395184191822\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.60it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 36.482113245105026\n","Validation perplexity: 36.482113\n","Epoch 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 24.781192485138135\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.65it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 30.42686051854323\n","Validation perplexity: 30.426861\n","Epoch 6\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.88it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 19.975010359931307\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.65it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 24.09215548001891\n","Validation perplexity: 24.092155\n","Epoch 7\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.42it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 14.178256448330295\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.65it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 15.96575728577981\n","Validation perplexity: 15.965757\n","Epoch 8\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.87it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.27it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 10.097060952148745\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.50it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 12.537885190707557\n","Validation perplexity: 12.537885\n","Epoch 9\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.85it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 8.041673375853048\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.67it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 10.167672190626027\n","Validation perplexity: 10.167672\n","Epoch 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 6.3707693740538565\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.76it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 8.53414204206735\n","Validation perplexity: 8.534142\n","Epoch 11\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.87it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.84it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 5.346270869216869\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.64it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 7.436633749346687\n","Validation perplexity: 7.436634\n","Epoch 12\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.87it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.57it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.636551228584552\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.39it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 6.738077019986307\n","Validation perplexity: 6.738077\n","Epoch 13\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.88it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.139951898358672\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.72it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 6.109578465705943\n","Validation perplexity: 6.109578\n","Epoch 14\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.87it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 3.747990331742934\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.59it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 5.562359923912533\n","Validation perplexity: 5.562360\n","Epoch 15\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.88it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 3.400493724109707\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.70it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 5.191183346838957\n","Validation perplexity: 5.191183\n","Epoch 16\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 3.160959319564019\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.64it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.915829559045294\n","Validation perplexity: 4.915830\n","Epoch 17\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.87it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.937889689410582\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.61it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.741284766207096\n","Validation perplexity: 4.741285\n","Epoch 18\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.7627043984451154\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.74it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.41464438651577\n","Validation perplexity: 4.414644\n","Epoch 19\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.5979389893193146\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.25753033179605\n","Validation perplexity: 4.257530\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[66.02021267971398,\n"," 51.30249561738432,\n"," 45.68239112190183,\n"," 41.78727966139444,\n"," 36.482113245105026,\n"," 30.42686051854323,\n"," 24.09215548001891,\n"," 15.96575728577981,\n"," 12.537885190707557,\n"," 10.167672190626027,\n"," 8.53414204206735,\n"," 7.436633749346687,\n"," 6.738077019986307,\n"," 6.109578465705943,\n"," 5.562359923912533,\n"," 5.191183346838957,\n"," 4.915829559045294,\n"," 4.741284766207096,\n"," 4.41464438651577,\n"," 4.25753033179605]"]},"metadata":{"tags":[]},"execution_count":188}]},{"cell_type":"code","metadata":{"id":"gKByZ0ltDEuM"},"source":["# Set to True if you want to save this model\n","# MAKE SURE TO SAVE YOUR MAPPINGS AS WELL WITH THE CELL EARLIER IN THE NOTEBOOK\n","save_model = False\n","if save_model:\n","    torch.save(simple_encoder_decoder.state_dict(), os.path.join(project_dir, \"simple_encoder_decoder.pt\"))\n","\n","# Set to True if you want to load a previously trained model\n","# MAKE SURE TO LOAD MODEL'S CORRESPONDING MAPPINGS WITH THE CELL EARLIER IN THE NOTEBOOK\n","load_model = False\n","if load_model:\n","    simple_encoder_decoder.load_state_dict(torch.load(os.path.join(project_dir, \"simple_encoder_decoder.pt\"), map_location=device))\n","    simple_encoder_decoder.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SFlXp0CM68yS"},"source":["# Decoding\n","Here we will use the trained model to decode input texts into generated code"]},{"cell_type":"code","metadata":{"id":"WJIUbA49DEuM","executionInfo":{"status":"ok","timestamp":1621302963471,"user_tz":240,"elapsed":393,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["def tokens_to_text(ids, mapping):\n","    # TODO: handle newlines and spaces\n","    return \"\".join([mapping[id] for id in ids])"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"BwUS4pRpDEuO"},"source":["# Read and tokenize test inputs and targets\n","test_raw_inputs = read_file(test_input_path)\n","test_raw_targets = read_file(test_target_path)\n","test_input_line_tokens = [e.tokens for e in input_tokenizer.encode_batch(test_raw_inputs)]\n","test_target_line_tokens = [e.tokens for e in target_tokenizer.encode_batch(test_raw_targets)]\n","\n","# Truncate line tokens so it matches training data\n","trunc_test_input_line_tokens = []\n","for line_tokens in test_input_line_tokens:\n","    if len(line_tokens) > input_max_seq_len - 2:\n","        trunc_test_input_line_tokens.append(line_tokens[:input_max_seq_len - 2])\n","    else:\n","        trunc_test_input_line_tokens.append(line_tokens)\n","        \n","trunc_test_target_line_tokens = []\n","for line_tokens in test_target_line_tokens:\n","    if len(line_tokens) > target_max_seq_len - 2:\n","        trunc_test_target_line_tokens.append(line_tokens[:target_max_seq_len - 2])\n","    else:\n","        trunc_test_target_line_tokens.append(line_tokens)\n","\n","simple_test_dataset = SimpleHearthstoneDataset(trunc_test_input_line_tokens, trunc_test_target_line_tokens, input_token2id, target_token2id, \"test\", input_max_seq_len=input_max_seq_len, target_max_seq_len=target_max_seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6DtzrSDfKbG0"},"source":["## Greedy Decoding\n","We first try decoding using a greedy approach, taking the most likely token at each step and expanding that sequence"]},{"cell_type":"code","metadata":{"id":"RA3odn1e6_ji"},"source":["def greedy_decoding(model, src_ids, src_lengths, max_len):\n","    \"\"\"Greedily decode a sentence for EncoderDecoder. Make sure to chop off the \n","         EOS token!\"\"\"\n","\n","    with torch.no_grad():\n","        encoder_outputs, encoder_hidden = model.encode(src_ids, src_lengths)\n","        prev_y = torch.ones(1, 1).fill_(SOS_ID).type_as(src_ids)\n","    \n","    output = []\n","    hidden = None\n","\n","    for i in range(max_len):\n","        with torch.no_grad():\n","            outputs, hidden = model.decode(prev_y, encoder_outputs, encoder_hidden, hidden)\n","            prob = model.generator(outputs[:, -1])\n","        d, next_word = torch.max(prob, dim=1)\n","        next_word = next_word.data.item()\n","        output.append(next_word)\n","        prev_y = torch.ones(1, 1).type_as(src_ids).fill_(next_word)\n","\n","    output = np.array(output)\n","\n","    # Cut off everything starting from </s>.\n","    first_eos = np.where(output == EOS_ID)[0]\n","    if len(first_eos) > 0:\n","        output = output[:first_eos[0]]\n","\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JrUTkDoxDEuM"},"source":["def spot_check_greedy(model, dataset, idx=None, n=1):\n","    \"\"\"Compare a (random) generated and target sequence using greedy search\"\"\"\n","    for i in range(n):\n","        if idx is None:\n","            idx = np.random.randint(0, len(dataset))\n","        inp_ids, trg_ids, inp_lens, trg_lens = dataset[idx: idx+1]\n","        greedy_decoded = greedy_decoding(model, inp_ids, inp_lens, target_max_seq_len)\n","        stripped_trg_ids = trg_ids[0][trg_ids[0] != PAD_ID][1:-1].tolist()\n","        stripped_inp_ids = inp_ids[0][inp_ids[0] != PAD_ID][1:-1].tolist()\n","        print(\"===============================\")\n","        print(f\"Input: {tokens_to_text(stripped_inp_ids, input_id2token)}\")\n","        print(f\"Expected:\\n\\n\\t{tokens_to_text(stripped_trg_ids, target_id2token)}\\n\\n-got-\\n\\n\\t{tokens_to_text(greedy_decoded, target_id2token)}\")\n","        print(\"===============================\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5p8H_uBeDEuN","outputId":"12676961-ecc1-45f5-8f87-56e30225bc9d"},"source":["# Spot check for training set\n","spot_check_greedy(simple_encoder_decoder, simple_train_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["===============================\n","Input: SacrificialPactNAME_END-1ATK_END-1DEF_END0COST_END-1DUR_ENDSpellTYPE_ENDWarlockPLAYER_CLS_ENDNILRACE_ENDCommonRARITY_ENDDestroyaDemon.Restore#5Healthtoyourhero.\n","Expected:\n","\n","\tclassSacrificialPact(SpellCard):§def__init__(self):§super().__init__(\"SacrificialPact\",0,CHARACTER_CLASS.WARLOCK,CARD_RARITY.COMMON,target_func=hearthbreaker.targeting.find_spell_target,filter_func=lambdacharacter:character.card.minion_type==MINION_TYPE.DEMON)§§defuse(self,player,game):§super().use(player,game)§self.target.die(self)§player.hero.heal(player.effective_heal_power(5),self)§\n","\n","-got-\n","\n","\tclassArcaneMissiles(SpellCard):§def__init__(self):§super().__init__(\"HolySpirit\",2,CHARACTER_CLASS.SHAMAN,CARD_RARITY.COMMON,target_func=hearthbreaker.targeting.find_minion_spell_target)§§defuse(self,player,game):§super().use(player,game)§self.target.damage(player.effective_spell_damage(2),self)§\n","===============================\n"],"name":"stdout"},{"output_type":"stream","text":["<ipython-input-14-e79693af7427>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attention_weights = F.softmax(attention_raw)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8116KQEBDEuO","outputId":"a79982d1-91fc-4e66-ce13-c6057f6dd4f2"},"source":["# Spot check for testing set\n","spot_check_greedy(simple_encoder_decoder, simple_test_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["===============================\n","Input: MadUNKBomberNAME_END5ATK_END4DEF_END5COST_END-1DUR_ENDMinionTYPE_ENDNeutralPLAYER_CLS_ENDNILRACE_ENDRareRARITY_END<b>Battlecry:</b>Deal6damagerandomlysplitbetweenallothercharacters.\n","Expected:\n","\n","\tclassMadUNKBomber(MinionCard):§def__init__(self):§super().__init__(\"MadUNKBomber\",5,CHARACTER_CLASS.ALL,CARD_RARITY.RARE,battlecry=Battlecry(Damage(1),CharacterSelector(players=BothPlayer(),picker=RandomPicker(6))))§§defcreate_minion(self,player):§returnMinion(5,4)§\n","\n","-got-\n","\n","\tclassDruidOfTheClaw(MinionCard):§def__init__(self):§super().__init__(\"Ancientof\",3,CHARACTER_CLASS.ALL,CARD_RARITY.COMMON)§§defcreate_minion(self,player):§returnMinion(2,2,effects=[Effect(TurnEnded(),ActionTag(Give(ChangeAttack(1)),SelfSelector()))])§\n","===============================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MOvRJNJaKqbE"},"source":["## Beam Search\n","We also try decoding using beam search, expanding the top k most likely sequences at each step"]},{"cell_type":"code","metadata":{"id":"oOuBBSsM7JkA"},"source":["def beam_search_decoding(model, src_ids, src_lengths, max_len, k=25):\n","    \"\"\"Keep expanding top k most likely sequences\"\"\"\n","    with torch.no_grad():\n","        encoder_outputs, encoder_hidden = model.encode(src_ids, src_lengths)\n","    \n","    # Keep track of top outputs stores as (log prob, output ID seq, hidden)\n","    top_outputs = [(0, [SOS_ID], None)]\n","\n","    for i in range(max_len):\n","        new_top_outputs = []\n","        for log_prob, output, hidden in top_outputs:\n","            # Get last token of candidate output sequence and use as input to decoder\n","            prev_y = torch.ones(1, 1).type_as(src_ids).fill_(output[-1])\n","            probs = None\n","            h = None\n","            with torch.no_grad():\n","                o, h = model.decode(prev_y, encoder_outputs, encoder_hidden, hidden)\n","                probs = model.generator(o[:, -1])\n","            # Get top k log probs and ids\n","            topk_log_probs, topk_ids = torch.topk(probs,k, dim=1)\n","            for token_log_prob, token_id in zip(topk_log_probs[0], topk_ids[0]):\n","                new_top_outputs.append((log_prob + token_log_prob.data.item(), output + [token_id.data.item()], h))\n","        # Get top k most likely output sequences up to this point\n","        new_top_outputs = sorted(new_top_outputs, key=lambda d: d[0], reverse=True)\n","        top_outputs = new_top_outputs[:k]\n","    \n","    # Get the most likely output sequence of all top outputs\n","    output = np.array(max(top_outputs, key=lambda d: d[0])[1])\n","\n","    # Cut off everything starting from </s>.\n","    first_eos = np.where(output == EOS_ID)[0]\n","    if len(first_eos) > 0:\n","        output = output[:first_eos[0]]\n","\n","    return output[1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dsezBoPCDEuN"},"source":["def spot_check_beam(model, dataset, idx=None, k=10):\n","    \"\"\"Compare a (random) generated and target sequence using beam search\"\"\"\n","    if idx is None:\n","        idx = np.random.randint(0, len(dataset))\n","    inp_ids, trg_ids, inp_lens, trg_lens = dataset[idx: idx+1]\n","    beam_decoded = beam_search_decoding(model, inp_ids, inp_lens, target_max_seq_len)\n","    stripped_trg_ids = trg_ids[0][trg_ids[0] != PAD_ID][1:-1].tolist()\n","    stripped_trg_ids = trg_ids[0][trg_ids[0] != PAD_ID][1:-1].tolist()\n","    stripped_inp_ids = inp_ids[0][inp_ids[0] != PAD_ID][1:-1].tolist()\n","    print(\"===============================\")\n","    print(f\"Input: {tokens_to_text(stripped_inp_ids, input_id2token)}\")\n","    print(f\"Expected:\\n\\n\\t{tokens_to_text(stripped_trg_ids, target_id2token)}\\n\\n-got-\\n\\n\\t{tokens_to_text(beam_decoded, target_id2token)}\")\n","    print(\"===============================\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vp_0v8vODEuN","outputId":"1348bbed-4a7e-4d02-dc26-5714a63db028"},"source":["# Spot check for training set\n","spot_check_beam(simple_encoder_decoder, simple_train_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["===============================\n","Input: CaptainGreenskinNAME_END5ATK_END4DEF_END5COST_END-1DUR_ENDMinionTYPE_ENDNeutralPLAYER_CLS_ENDPirateRACE_ENDLegendaryRARITY_END<b>Battlecry:</b>Giveyourweapon+1/+1.\n","Expected:\n","\n","\tclassCaptainGreenskin(MinionCard):§def__init__(self):§super().__init__(\"CaptainGreenskin\",5,CHARACTER_CLASS.ALL,CARD_RARITY.LEGENDARY,minion_type=MINION_TYPE.PIRATE,battlecry=Battlecry([IncreaseWeaponAttack(1),IncreaseDurability()],WeaponSelector()))§§defcreate_minion(self,player):§returnMinion(5,4)§\n","\n","-got-\n","\n","\tclassDruidOfTheClaw(MinionCard):§def__init__(self):§super().__init__(\"AncientTotem\",3,CHARACTER_CLASS.ALL,CARD_RARITY.COMMON,minion_type=MINION_TYPE.MECH)§§defcreate_minion(self,player):§returnMinion(2,3,effects=[Effect(TurnStarted(),ActionTag(Give(ChangeAttack(1)),PlayerSelector()))])§\n","===============================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"luo2ffDeDEuO","outputId":"bb429322-f0aa-4ebe-bf4b-d8041cb85d5a"},"source":["# Spot check for testing set\n","spot_check_beam(simple_encoder_decoder, simple_test_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["===============================\n","Input: UNKUNKUNKVUNKUNKNAME_END-1ATK_END-1DEF_END5COST_END-1DUR_ENDSpellTYPE_ENDPaladinPLAYER_CLS_ENDNILRACE_ENDCommonRARITY_ENDDraw2cards.Costs(1)lessforeachminionthatdiedthisturn.\n","Expected:\n","\n","\tclassUNKUNKUNKUNKVUNKUNK(SpellCard):§def__init__(self):§super().__init__(\"UNKUNKUNKUNKVUNKUNK\",5,CHARACTER_CLASS.PALADIN,CARD_RARITY.COMMON,buffs=[Buff(ManaChange(Count(DeadMinionSelector(players=BothPlayer())),-1))])§§defuse(self,player,game):§super().use(player,game)§forUNKinrange(0,2):§player.draw()§\n","\n","-got-\n","\n","\tclassLightOfTheNaaru(SpellCard):§def__init__(self):§super().__init__(\"Arcaneofs\",1,CHARACTER_CLASS.SHAMAN,CARD_RARITY.COMMON,target_func=hearthbreaker.targeting.find_minion_spell_target)§§defuse(self,player,game):§super().use(player,game)§self.target.damage(player.effective_spell_damage(2),self)§\n","===============================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d4LdBIRChwhq"},"source":["# Evaluation\n","We evaluate our models using accuracy (exact match) and BLEU scores"]},{"cell_type":"code","metadata":{"id":"5tNI3acthulj"},"source":["def evaluate_accuracy(model, test_dataset, decoder, pad_token):\n","    \"\"\"\n","    :param model: model to evaluate\n","    :param test_dataset: test dataset to evaluate that yields (input, target_tokens, input_length (or empty value), target_length (or empty value))\n","            target_tokens should have sequence that starts and ends with SOS and EOS tokens respectively and may be padded with pad_token\n","    :param decoder: decoder to evaluate with; returns a list of predicted tokens with SOS, EOS, and PAD tokens removed\n","    :param pad_token: padding token used in target_tokens\n","    \"\"\"\n","    matches = []\n","    for i in tqdm(range(len(test_dataset)), position=0, leave=True):\n","        inp, trg_tokens, inp_len, trg_len = test_dataset[i: i+1]\n","        trunc_trg_tokens = trg_tokens[0][trg_tokens[0] != pad_token][1:-1].tolist()\n","\n","        pred_tokens = decoder(model, inp, inp_len, trg_len)\n","        \n","        matches.append(1 if pred_tokens == trg_tokens else 0)\n","    return matches"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H7L8IBLsk4RY"},"source":["def evaluate_bleu(model, test_dataset, decoder, token2str, pad_token):\n","    \"\"\"\n","    :param model: model to evaluate\n","    :param test_dataset: test dataset to evaluate that yields (input, target_tokens, input_length (or empty value), target_length (or empty value))\n","            target_tokens should have sequence that starts and ends with SOS and EOS tokens respectively and may be padded with pad_token\n","    :param decoder: decoder to evaluate with; returns a list of predicted tokens with SOS, EOS, and PAD tokens removed\n","    :param token2str: mapping from token to string\n","    :param pad_token: padding token used in target_tokens\n","    \"\"\"\n","    bleu_scores = []\n","    for i in tqdm(range(len(test_dataset)), position=0, leave=True):\n","        inp, trg_tokens, inp_len, trg_len = test_dataset[i: i+1]\n","        trunc_trg_tokens = trg_tokens[0][trg_tokens[0] != pad_token][1:-1].tolist()\n","\n","        pred_tokens = decoder(model, inp, inp_len, trg_len)\n","        pred_text = \" \".join([token2str[t] for t in pred_tokens])\n","        trg_text = \" \".join([token2str[t] for t in trunc_trg_tokens])\n","\n","        bleu_scores.append(sacrebleu.raw_corpus_bleu([pred_text], [[trg_text]], 0.01).score)\n","\n","    return bleu_scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ySNzNHIl5knR"},"source":["## Simple Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t-yYwbjkx-QC","outputId":"2bdcc6f1-212d-499e-e498-9bea92d5c644"},"source":["# Beam Search\n","simple_beam_matches = evaluate_accuracy(simple_encoder_decoder, simple_test_dataset, beam_search_decoding, PAD_ID)\n","simple_beam_bleus = evaluate_bleu(simple_encoder_decoder, simple_test_dataset, beam_search_decoding, target_id2token, PAD_ID)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 0/66 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","100%|██████████| 66/66 [03:08<00:00,  2.85s/it]\n","100%|██████████| 66/66 [03:07<00:00,  2.84s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iZ4WwVaaywWy","outputId":"65f8c43f-dc43-47a5-cafb-61ef252fd29f"},"source":["# Greedy Search\n","simple_greedy_matches = evaluate_accuracy(simple_encoder_decoder, simple_test_dataset, greedy_decoding, PAD_ID)\n","simple_greedy_bleus = evaluate_bleu(simple_encoder_decoder, simple_test_dataset, greedy_decoding, target_id2token, PAD_ID)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 0/66 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","100%|██████████| 66/66 [00:04<00:00, 15.17it/s]\n","100%|██████████| 66/66 [00:04<00:00, 14.99it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vDRTIdBN5axk","outputId":"eb836d04-6c45-48c0-dbae-7b010491221c"},"source":["print(\"Metrics for Simple UNK Encoder/Decoder\")\n","print(f\"Beam Accuracy: {sum(simple_beam_matches) / len(simple_beam_matches)}\")\n","print(f\"Beam BLEU: {sum(simple_beam_bleus) / len(simple_beam_bleus)}\\n\")\n","print(f\"Greedy Accuracy: {sum(simple_greedy_matches) / len(simple_greedy_matches)}\")\n","print(f\"Greedy BLEU: {sum(simple_greedy_bleus) / len(simple_greedy_bleus)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Metrics for Simple UNK Encoder/Decoder\n","Beam Accuracy: 0.0\n","Beam BLEU: 42.81463596876857\n","\n","Greedy Accuracy: 0.0\n","Greedy BLEU: 42.900648880536515\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iu-lMrLSB-VX"},"source":["# Enforced UNK Tokens\n","Our simple model may be achieving 0 accuracy because it cannot handle UNK tokens due to the training data not having UNK tokens. Here we will try to train the simple model again, this time with the training data tokenized probabilistically to be replaced with UNK with probability p."]},{"cell_type":"code","metadata":{"id":"IXgZvlc1Dtsg"},"source":["def tokens_to_ids_enforced_unk(tokens, token2id, mode, max_length, unk_prob=0.5):\n","    \"\"\"Convert list of tokens to IDs\"\"\"\n","    token_ids = []\n","    for token in tokens:\n","        if token in token2id:\n","            if mode == \"train\":\n","                if np.random.random() >= unk_prob:\n","                    token_ids.append(token2id[token])\n","                else:    \n","                    token_ids.append(token2id[\"UNK\"])\n","            else:\n","                token_ids.append(token2id[token])\n","        elif mode == \"train\":\n","            token_ids.append(len(token2id))\n","            token2id[token] = len(token2id)\n","        else:\n","            token_ids.append(token2id[\"UNK\"])\n","    ids_len = len(token_ids) + 2 \n","    token_ids = [token2id[\"SOS_INDEX\"]] + token_ids + \\\n","                   [token2id[\"EOS_INDEX\"]] + \\\n","                   [token2id[\"PAD_INDEX\"]]*(max_length - ids_len)\n","    return token_ids, token2id, ids_len\n","\n","def batch_tokens_to_ids_enforced_unk(line_tokens, token2id, mode, max_length=None, unk_prob=0.5):\n","    \"\"\"Convert tokens to IDs for a batch and returns long tensor for whole batch of IDs\"\"\"\n","    if max_length is None:\n","        max_length = max([len(line) for line in line_tokens])\n","    line_ids = []\n","    line_lens = []\n","    for tokens in line_tokens:\n","        ids, token2id, line_len = tokens_to_ids_enforced_unk(tokens, token2id, mode, max_length=max_length, unk_prob=unk_prob)\n","        line_ids.append(ids)\n","        line_lens.append(line_len)\n","    return torch.LongTensor(line_ids).to(device), token2id, torch.LongTensor(line_lens).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iRFCBAVcCjxp"},"source":["class EnforcedUNKHearthstoneDataset(data.Dataset):\n","    \"\"\"Simple dataset with forced UNK tokens\"\"\"\n","    def __init__(self, input_line_tokens, target_line_tokens, input_token2id, target_token2id, mode, input_max_seq_len=None, target_max_seq_len=None, unk_prob=0.5):\n","        self.input_line_ids, _, self.input_line_lens = batch_tokens_to_ids_enforced_unk(input_line_tokens, input_token2id, mode, max_length=input_max_seq_len, unk_prob=unk_prob)\n","        self.target_line_ids, _, self.target_line_lens = batch_tokens_to_ids_enforced_unk(target_line_tokens, target_token2id, mode, max_length=target_max_seq_len, unk_prob=unk_prob)\n","        \n","    def __len__(self):\n","        return len(self.input_line_ids)\n","    \n","    def __getitem__(self, idx):\n","        return self.input_line_ids[idx], self.target_line_ids[idx], self.input_line_lens[idx], self.target_line_lens[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6bt2PquEFmcr"},"source":["unk_prob = 0.1\n","\n","# Split validation and training data\n","validation_ratio = 0.1\n","train_size = int((1 - validation_ratio) * len(input_line_tokens))\n","train_input_line_tokens = input_line_tokens[: train_size]\n","train_target_line_tokens = target_line_tokens[: train_size]\n","validation_input_line_tokens = input_line_tokens[train_size:]\n","validation_target_line_tokens = target_line_tokens[train_size:]\n","\n","# Create datasets for validation and training\n","enforced_unk_train_dataset = EnforcedUNKHearthstoneDataset(train_input_line_tokens, train_target_line_tokens, input_token2id, target_token2id, \"train\", input_max_seq_len=input_max_seq_len, target_max_seq_len=target_max_seq_len, unk_prob=unk_prob)\n","enforced_unk_validation_dataset = EnforcedUNKHearthstoneDataset(validation_input_line_tokens, validation_target_line_tokens, input_token2id, target_token2id, \"train\", input_max_seq_len=input_max_seq_len, target_max_seq_len=target_max_seq_len, unk_prob=unk_prob)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZIGReuKCFg8x"},"source":["# Set params\n","input_embedding_size = 256\n","input_hidden_size = 256\n","target_embedding_size = 256\n","target_hidden_size = 256\n","batch_size = 8\n","\n","# Create data objects\n","enforced_unk_train_dataloader = data.DataLoader(enforced_unk_train_dataset, batch_size=batch_size, shuffle=True)\n","enforced_unk_validation_dataloader = data.DataLoader(enforced_unk_validation_dataset, batch_size=batch_size, shuffle=True)\n","input_vocab_size = len(input_token2id)\n","target_vocab_size = len(target_token2id)\n","\n","# Create models\n","enforced_unk_encoder = SimpleHearthstoneEncoder(input_vocab_size, input_embedding_size, input_hidden_size).to(device)\n","enforced_unk_decoder = SimpleHearthstoneDecoder(target_vocab_size, target_embedding_size, target_hidden_size, 2 * input_hidden_size, input_max_seq_len).to(device)\n","enforced_unk_generator = Generator(target_hidden_size, target_vocab_size)\n","enforced_unk_encoder_decoder = SimpleHearthstoneEncoderDecoder(enforced_unk_encoder, enforced_unk_decoder, enforced_unk_generator).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dnKU13JXGSy5","outputId":"e513e35a-fdd6-417e-9386-7df797618fe8"},"source":["# Train model\n","epochs = 20\n","lr = 1e-3\n","\n","train(enforced_unk_encoder_decoder, enforced_unk_train_dataloader, enforced_unk_validation_dataloader, epochs, lr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","100%|██████████| 60/60 [00:31<00:00,  1.92it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 92.77032767430049\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.80it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 64.59443659079196\n","Validation perplexity: 64.594437\n","Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.92it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 48.891635205487574\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.75it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 51.11495101568877\n","Validation perplexity: 51.114951\n","Epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.92it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 39.58172748103038\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.68it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 45.10766823932047\n","Validation perplexity: 45.107668\n","Epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.91it/s]\n"," 14%|█▍        | 1/7 [00:00<00:00,  6.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 34.66082653726583\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.66it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 40.62151746055084\n","Validation perplexity: 40.621517\n","Epoch 4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.92it/s]\n"," 14%|█▍        | 1/7 [00:00<00:00,  6.08it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 31.303155629711213\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.73it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 38.234697182824284\n","Validation perplexity: 38.234697\n","Epoch 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.91it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 29.403740340650828\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.81it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 37.95302318738754\n","Validation perplexity: 37.953023\n","Epoch 6\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.91it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 27.342854793617203\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.77it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 35.35450890742274\n","Validation perplexity: 35.354509\n","Epoch 7\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.91it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 25.692043881454772\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.77it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 35.300711434534136\n","Validation perplexity: 35.300711\n","Epoch 8\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.91it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 24.736581774388515\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.68it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 34.07126083265607\n","Validation perplexity: 34.071261\n","Epoch 9\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.91it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 23.38962841131841\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.86it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 31.867095242302625\n","Validation perplexity: 31.867095\n","Epoch 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.92it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 22.935115550322706\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.75it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 31.88045793660857\n","Validation perplexity: 31.880458\n","Epoch 11\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.92it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 23.694296748085513\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.73it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 32.179020954045555\n","Validation perplexity: 32.179021\n","Epoch 12\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.92it/s]\n"," 14%|█▍        | 1/7 [00:00<00:00,  6.10it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 21.569570093892345\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.78it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 28.929138871489776\n","Validation perplexity: 28.929139\n","Epoch 13\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.91it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 20.089082940110465\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.81it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 29.224560952621612\n","Validation perplexity: 29.224561\n","Epoch 14\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.92it/s]\n"," 14%|█▍        | 1/7 [00:00<00:00,  6.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 18.31020873646927\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.90it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 26.314070551501736\n","Validation perplexity: 26.314071\n","Epoch 15\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.92it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 18.510124516125533\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.76it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 26.767510583979487\n","Validation perplexity: 26.767511\n","Epoch 16\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.90it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 17.466616161335683\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.34it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 25.515796022626485\n","Validation perplexity: 25.515796\n","Epoch 17\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.91it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 16.647960925848313\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.80it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 25.311507377062533\n","Validation perplexity: 25.311507\n","Epoch 18\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.92it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 17.43338158722167\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.78it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 25.215000358228714\n","Validation perplexity: 25.215000\n","Epoch 19\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.92it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 16.805323656696036\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.72it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 25.660071331196782\n","Validation perplexity: 25.660071\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[64.59443659079196,\n"," 51.11495101568877,\n"," 45.10766823932047,\n"," 40.62151746055084,\n"," 38.234697182824284,\n"," 37.95302318738754,\n"," 35.35450890742274,\n"," 35.300711434534136,\n"," 34.07126083265607,\n"," 31.867095242302625,\n"," 31.88045793660857,\n"," 32.179020954045555,\n"," 28.929138871489776,\n"," 29.224560952621612,\n"," 26.314070551501736,\n"," 26.767510583979487,\n"," 25.515796022626485,\n"," 25.311507377062533,\n"," 25.215000358228714,\n"," 25.660071331196782]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"L6DWtjF-NZ1Y"},"source":["# Set to True if you want to save this model\n","# MAKE SURE TO SAVE YOUR MAPPINGS AS WELL WITH THE CELL EARLIER IN THE NOTEBOOK\n","save_model = False\n","if save_model:\n","    torch.save(enforced_unk_encoder_decoder.state_dict(), os.path.join(project_dir, \"enforced_unk_encoder_decoder.pt\"))\n","\n","# Set to True if you want to load a previously trained model\n","# MAKE SURE TO LOAD MODEL'S CORRESPONDING MAPPINGS WITH THE CELL EARLIER IN THE NOTEBOOK\n","load_model = False\n","if load_model:\n","    enforced_unk_encoder_decoder.load_state_dict(torch.load(os.path.join(project_dir, \"enforced_unk_encoder_decoder.pt\"), map_location=device))\n","    enforced_unk_encoder_decoder.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"riM6ANtGJeiq"},"source":["## Evaluate Model"]},{"cell_type":"code","metadata":{"id":"qFvuHFo7Jg9E"},"source":["# Read and tokenize test inputs and targets\n","test_raw_inputs = read_file(test_input_path)\n","test_raw_targets = read_file(test_target_path)\n","test_input_line_tokens = [e.tokens for e in input_tokenizer.encode_batch(test_raw_inputs)]\n","test_target_line_tokens = [e.tokens for e in target_tokenizer.encode_batch(test_raw_targets)]\n","\n","# Truncate line tokens so it matches training data\n","trunc_test_input_line_tokens = []\n","for line_tokens in test_input_line_tokens:\n","    if len(line_tokens) > input_max_seq_len - 2:\n","        trunc_test_input_line_tokens.append(line_tokens[:input_max_seq_len - 2])\n","    else:\n","        trunc_test_input_line_tokens.append(line_tokens)\n","        \n","trunc_test_target_line_tokens = []\n","for line_tokens in test_target_line_tokens:\n","    if len(line_tokens) > target_max_seq_len - 2:\n","        trunc_test_target_line_tokens.append(line_tokens[:target_max_seq_len - 2])\n","    else:\n","        trunc_test_target_line_tokens.append(line_tokens)\n","\n","simple_test_dataset = SimpleHearthstoneDataset(trunc_test_input_line_tokens, trunc_test_target_line_tokens, input_token2id, target_token2id, \"test\", input_max_seq_len=input_max_seq_len, target_max_seq_len=target_max_seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qsbSVh0_JmbT","outputId":"0c1e0159-30d0-40dc-9034-f457b6c5d1b4"},"source":["spot_check_greedy(enforced_unk_encoder_decoder, simple_test_dataset)\n","# spot_check_beam(enforced_unk_encoder_decoder, simple_test_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["===============================\n","Input: SummonUNKPortalNAME_END0ATK_END4DEF_END4COST_END-1DUR_ENDMinionTYPE_ENDWarlockPLAYER_CLS_ENDNILRACE_ENDCommonRARITY_ENDYourminionscost(2)less,bUNKUNKUNKlessUNK(1).\n","Expected:\n","\n","\tclassSummonUNKPortal(MinionCard):§def__init__(self):§super().__init__(\"SummonUNKPortal\",4,CHARACTER_CLASS.WARLOCK,CARD_RARITY.COMMON)§§defcreate_minion(self,player):§returnMinion(0,4,auras=[Aura(ManaChange(-2,1,mUNKUNK=1),CardSelector(condition=IsMinion()))])§\n","\n","-got-\n","\n","\tclassUNK(MinionCard):§def__init__(self):§super().__init__(\"UNKUNKUNK\",\",,,CHARACTER_CLASS.ALLCARD_RARITY.COMMON,defdef((self((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((\n","===============================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QfNB-oB3KAK7","outputId":"cb62f4cd-bb52-4a67-ea00-c28e7419bdcc"},"source":["# Beam Search\n","enforced_unk_beam_matches = evaluate_accuracy(enforced_unk_encoder_decoder, simple_test_dataset, beam_search_decoding, PAD_ID)\n","enforced_unk_beam_bleus = evaluate_bleu(enforced_unk_encoder_decoder, simple_test_dataset, beam_search_decoding, target_id2token, PAD_ID)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 0/66 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","100%|██████████| 66/66 [03:04<00:00,  2.79s/it]\n","100%|██████████| 66/66 [03:03<00:00,  2.79s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ihUn5qeKRKi","outputId":"e99fc48a-dd69-4d8a-ced8-cf3ae388633d"},"source":["# Greedy Search\n","enforced_unk_greedy_matches = evaluate_accuracy(enforced_unk_encoder_decoder, simple_test_dataset, greedy_decoding, PAD_ID)\n","enforced_unk_greedy_bleus = evaluate_bleu(enforced_unk_encoder_decoder, simple_test_dataset, greedy_decoding, target_id2token, PAD_ID)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 0/66 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","100%|██████████| 66/66 [00:04<00:00, 15.12it/s]\n","100%|██████████| 66/66 [00:04<00:00, 14.98it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9izIzrrfRh0T","outputId":"300c1d20-aeff-4984-cdd1-6527e212b588"},"source":["print(\"Metrics for Enforced UNK Encoder/Decoder\")\n","print(f\"Beam Accuracy: {sum(enforced_unk_beam_matches) / len(enforced_unk_beam_matches)}\")\n","print(f\"Beam BLEU: {sum(enforced_unk_beam_bleus) / len(enforced_unk_beam_bleus)}\\n\")\n","print(f\"Greedy Accuracy: {sum(enforced_unk_greedy_matches) / len(enforced_unk_greedy_matches)}\")\n","print(f\"Greedy BLEU: {sum(enforced_unk_greedy_bleus) / len(enforced_unk_greedy_bleus)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Metrics for Enforced UNK Encoder/Decoder\n","Beam Accuracy: 0.0\n","Beam BLEU: 28.52484777391203\n","\n","Greedy Accuracy: 0.0\n","Greedy BLEU: 27.989055259631616\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NPKLX0W5Oh10"},"source":["## Alternative Approach\n","Instead of replacing tokens in a sequence with random UNK, we replace entire tokens with UNK throughout all sequences (from input to output)"]},{"cell_type":"code","metadata":{"id":"E4T4cjVTSj0E"},"source":["unk_prob = 0.1\n","\n","# Choose dropped tokens\n","all_tokens = set(input_token2id.keys()).union(set(target_token2id.keys()))\n","dropped_tokens = set()\n","for token in all_tokens:\n","    if np.random.random() < unk_prob:\n","        dropped_tokens.add(token)\n","\n","# Drop tokens from input and target mappings\n","dropped_input_token2id = {\n","    token: (id if token not in dropped_tokens else input_token2id[\"UNK\"])\n","    for (token, id) in input_token2id.items()\n","}\n","\n","dropped_target_token2id = {\n","    token: (id if token not in dropped_tokens else target_token2id[\"UNK\"])\n","    for (token, id) in target_token2id.items()\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tXPr6buiQmVq"},"source":["# Split validation and training data\n","validation_ratio = 0.1\n","train_size = int((1 - validation_ratio) * len(input_line_tokens))\n","train_input_line_tokens = input_line_tokens[: train_size]\n","train_target_line_tokens = target_line_tokens[: train_size]\n","validation_input_line_tokens = input_line_tokens[train_size:]\n","validation_target_line_tokens = target_line_tokens[train_size:]\n","\n","# Create datasets for validation and training\n","enforced_unk_train_dataset_v2 = SimpleHearthstoneDataset(train_input_line_tokens, train_target_line_tokens, dropped_input_token2id, dropped_target_token2id, \"train\", input_max_seq_len=input_max_seq_len, target_max_seq_len=target_max_seq_len)\n","enforced_unk_validation_dataset_v2 = SimpleHearthstoneDataset(validation_input_line_tokens, validation_target_line_tokens, dropped_input_token2id, dropped_target_token2id, \"train\", input_max_seq_len=input_max_seq_len, target_max_seq_len=target_max_seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yl-XgeLRQuPm"},"source":["# Set params\n","input_embedding_size = 256\n","input_hidden_size = 256\n","target_embedding_size = 256\n","target_hidden_size = 256\n","batch_size = 8\n","\n","# Create data objects\n","enforced_unk_train_dataloader_v2 = data.DataLoader(enforced_unk_train_dataset_v2, batch_size=batch_size, shuffle=True)\n","enforced_unk_validation_dataloader_v2 = data.DataLoader(enforced_unk_validation_dataset_v2, batch_size=batch_size, shuffle=True)\n","input_vocab_size = len(input_token2id)\n","target_vocab_size = len(target_token2id)\n","\n","# Create models\n","enforced_unk_encoder_v2 = SimpleHearthstoneEncoder(input_vocab_size, input_embedding_size, input_hidden_size).to(device)\n","enforced_unk_decoder_v2 = SimpleHearthstoneDecoder(target_vocab_size, target_embedding_size, target_hidden_size, 2 * input_hidden_size, input_max_seq_len).to(device)\n","enforced_unk_generator_v2 = Generator(target_hidden_size, target_vocab_size)\n","enforced_unk_encoder_decoder_v2 = SimpleHearthstoneEncoderDecoder(enforced_unk_encoder_v2, enforced_unk_decoder_v2, enforced_unk_generator_v2).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"exUeKW0xQ7IF","outputId":"9024efe1-e4ad-4f28-a106-3e0b90415ca5"},"source":["# Train model\n","epochs = 20\n","lr = 1e-3\n","\n","train(enforced_unk_encoder_decoder_v2, enforced_unk_train_dataloader_v2, enforced_unk_validation_dataloader_v2, epochs, lr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","100%|██████████| 60/60 [00:31<00:00,  1.90it/s]\n"," 14%|█▍        | 1/7 [00:00<00:00,  6.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 86.99063554375284\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.77it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 55.93571853527773\n","Validation perplexity: 55.935719\n","Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.89it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.78it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 43.19354521168168\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.74it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 42.52581145639964\n","Validation perplexity: 42.525811\n","Epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.90it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 33.871814307446826\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.54it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 37.51456801933417\n","Validation perplexity: 37.514568\n","Epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.73it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 28.973027322808345\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.69it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 33.54435115696399\n","Validation perplexity: 33.544351\n","Epoch 4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.85it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 25.5705341044714\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.55it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 30.02757694986674\n","Validation perplexity: 30.027577\n","Epoch 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 22.495538624085615\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.62it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 26.165340942865896\n","Validation perplexity: 26.165341\n","Epoch 6\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.85it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 17.797076533503574\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.68it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 20.305135698322616\n","Validation perplexity: 20.305136\n","Epoch 7\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 13.774245278179588\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.62it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 15.337506187230248\n","Validation perplexity: 15.337506\n","Epoch 8\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.85it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 10.406968874387482\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.61it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 11.057919975613332\n","Validation perplexity: 11.057920\n","Epoch 9\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 6.795201434413\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.62it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 7.6366299014346435\n","Validation perplexity: 7.636630\n","Epoch 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.969993699617301\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.68it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 5.974474289644345\n","Validation perplexity: 5.974474\n","Epoch 11\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.85it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.84it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.07069735218965\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.59it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 5.290117000226588\n","Validation perplexity: 5.290117\n","Epoch 12\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 3.5759226856719555\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.58it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.854024420297158\n","Validation perplexity: 4.854024\n","Epoch 13\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:00,  6.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 3.2191803802486065\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.70it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.384808802926134\n","Validation perplexity: 4.384809\n","Epoch 14\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.85it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.9456561541236947\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.67it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.153415953208622\n","Validation perplexity: 4.153416\n","Epoch 15\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.746035385736981\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.58it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.0123031717407995\n","Validation perplexity: 4.012303\n","Epoch 16\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.595272552695728\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.69it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 3.811992156954705\n","Validation perplexity: 3.811992\n","Epoch 17\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  6.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.4350947595766734\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.64it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 3.7009473205194627\n","Validation perplexity: 3.700947\n","Epoch 18\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.3192863135169977\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.68it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 3.668884895560412\n","Validation perplexity: 3.668885\n","Epoch 19\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:32<00:00,  1.86it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.2279127879187373\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 3.6172720972582355\n","Validation perplexity: 3.617272\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[55.93571853527773,\n"," 42.52581145639964,\n"," 37.51456801933417,\n"," 33.54435115696399,\n"," 30.02757694986674,\n"," 26.165340942865896,\n"," 20.305135698322616,\n"," 15.337506187230248,\n"," 11.057919975613332,\n"," 7.6366299014346435,\n"," 5.974474289644345,\n"," 5.290117000226588,\n"," 4.854024420297158,\n"," 4.384808802926134,\n"," 4.153415953208622,\n"," 4.0123031717407995,\n"," 3.811992156954705,\n"," 3.7009473205194627,\n"," 3.668884895560412,\n"," 3.6172720972582355]"]},"metadata":{"tags":[]},"execution_count":86}]},{"cell_type":"code","metadata":{"id":"8WRYWfZZZF5e"},"source":["# Set to True if you want to save this model\n","# MAKE SURE TO SAVE YOUR MAPPINGS AS WELL WITH THE CELL EARLIER IN THE NOTEBOOK\n","save_model = False\n","if save_model:\n","    torch.save(enforced_unk_encoder_decoder_v2.state_dict(), os.path.join(project_dir, \"enforced_unk_encoder_decoder_v2.pt\"))\n","\n","# Set to True if you want to load a previously trained model\n","# MAKE SURE TO LOAD MODEL'S CORRESPONDING MAPPINGS WITH THE CELL EARLIER IN THE NOTEBOOK\n","load_model = False\n","if load_model:\n","    enforced_unk_encoder_decoder_v2.load_state_dict(torch.load(os.path.join(project_dir, \"enforced_unk_encoder_decoder_v2.pt\"), map_location=device))\n","    enforced_unk_encoder_decoder_v2.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gicTMDOMVczF","outputId":"bfec42fa-4025-45fb-a0fb-3ff8e370b472"},"source":["spot_check_greedy(enforced_unk_encoder_decoder_v2, simple_test_dataset)\n","# spot_check_beam(enforced_unk_encoder_decoder, simple_test_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["===============================\n","Input: UNKUNKUNKNAME_END-1ATK_END-1DEF_END1COST_END-1DUR_ENDSpellTYPE_ENDRoguePLAYER_CLS_ENDNILRACE_ENDCommonRARITY_ENDGiveyourminions<b>Stealth</b>untilyournextturn.\n","Expected:\n","\n","\tclassUNKUNKUNK(SpellCard):§def__init__(self):§super().__init__(\"UNKUNKUNK\",1,CHARACTER_CLASS.ROGUE,CARD_RARITY.COMMON)§§defuse(self,player,game):§super().use(player,game)§forminioninplayer.minions:§ifnotminion.stealth:§minion.add_buff(BuffUntil(Stealth(),TurnStarted()))§\n","\n","-got-\n","\n","\tUNKUNK(MinionCard):§def__init__(self):§super().__init__(\"UNKUNK\",UNK,CHARACTER_CLASS.ALL,CARD_RARITY.COMMON)§§defcreate_minion(self,player):§returnMinion(UNK,UNK,UNKUNKTrue)§\n","===============================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bsHZwru9VsYN","outputId":"99b3515c-ca84-48a2-8f5c-5b24c60e2427"},"source":["# Beam Search\n","enforced_unk_v2_beam_matches = evaluate_accuracy(enforced_unk_encoder_decoder_v2, simple_test_dataset, beam_search_decoding, PAD_ID)\n","enforced_unk_v2_beam_bleus = evaluate_bleu(enforced_unk_encoder_decoder_v2, simple_test_dataset, beam_search_decoding, target_id2token, PAD_ID)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 0/66 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","100%|██████████| 66/66 [03:04<00:00,  2.79s/it]\n","100%|██████████| 66/66 [03:03<00:00,  2.78s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_7EODceV0yM","outputId":"eb8fc580-0d7d-496d-b74c-ffebfb6fdc1c"},"source":["# Greedy Search\n","enforced_unk_v2_greedy_matches = evaluate_accuracy(enforced_unk_encoder_decoder_v2, simple_test_dataset, greedy_decoding, PAD_ID)\n","enforced_unk_v2_greedy_bleus = evaluate_bleu(enforced_unk_encoder_decoder_v2, simple_test_dataset, greedy_decoding, target_id2token, PAD_ID)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 0/66 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","100%|██████████| 66/66 [00:04<00:00, 15.17it/s]\n","100%|██████████| 66/66 [00:04<00:00, 15.02it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cEaBR__KYORv","outputId":"f27cfeb5-1c01-4910-b921-af90d22e16f6"},"source":["print(\"Metrics for Enforced UNK Encoder/Decoder v2\")\n","print(f\"Beam Accuracy: {sum(enforced_unk_beam_matches) / len(enforced_unk_beam_matches)}\")\n","print(f\"Beam BLEU: {sum(enforced_unk_beam_bleus) / len(enforced_unk_beam_bleus)}\\n\")\n","print(f\"Greedy Accuracy: {sum(enforced_unk_greedy_matches) / len(enforced_unk_greedy_matches)}\")\n","print(f\"Greedy BLEU: {sum(enforced_unk_greedy_bleus) / len(enforced_unk_greedy_bleus)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Metrics for Enforced UNK Encoder/Decoder v2\n","Beam Accuracy: 0.0\n","Beam BLEU: 28.52484777391203\n","\n","Greedy Accuracy: 0.0\n","Greedy BLEU: 27.989055259631616\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cx6KrZyKWAGF"},"source":["## Yet Another Approach\n","Here we will try yet another approach, replacing the least frequent tokens with UNK instead of choosing tokens at random."]},{"cell_type":"code","metadata":{"id":"Hbx9cT2rWK8k"},"source":["unk_ratio = 0.25\n","token_counts = {}\n","all_line_tokens = input_line_tokens + target_line_tokens\n","\n","# Get all the token counts between input and target\n","for line in all_line_tokens:\n","    for token in line:\n","        token_counts[token] = token_counts.get(token, 0) + 1\n","\n","# Drop the least used tokens\n","tokens_sorted_by_count = [token for (token, count) in sorted(token_counts.items(), key=lambda i: i[1])]\n","dropped_tokens = set(tokens_sorted_by_count[:int(unk_ratio * len(tokens_sorted_by_count))])\n","\n","# Drop tokens from input and target mappings\n","dropped_input_token2id_v3 = {\n","    token: (id if token not in dropped_tokens else input_token2id[\"UNK\"])\n","    for (token, id) in input_token2id.items()\n","}\n","\n","dropped_target_token2id_v3 = {\n","    token: (id if token not in dropped_tokens else target_token2id[\"UNK\"])\n","    for (token, id) in target_token2id.items()\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQEVcG1zWPDV"},"source":["# Split validation and training data\n","validation_ratio = 0.1\n","train_size = int((1 - validation_ratio) * len(input_line_tokens))\n","train_input_line_tokens = input_line_tokens[: train_size]\n","train_target_line_tokens = target_line_tokens[: train_size]\n","validation_input_line_tokens = input_line_tokens[train_size:]\n","validation_target_line_tokens = target_line_tokens[train_size:]\n","\n","# Create datasets for validation and training\n","enforced_unk_train_dataset_v3 = SimpleHearthstoneDataset(train_input_line_tokens, train_target_line_tokens, dropped_input_token2id_v3, dropped_target_token2id_v3, \"train\", input_max_seq_len=input_max_seq_len, target_max_seq_len=target_max_seq_len)\n","enforced_unk_validation_dataset_v3 = SimpleHearthstoneDataset(validation_input_line_tokens, validation_target_line_tokens, dropped_input_token2id_v3, dropped_target_token2id_v3, \"train\", input_max_seq_len=input_max_seq_len, target_max_seq_len=target_max_seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nBttULElYWSv"},"source":["# Set params\n","input_embedding_size = 256\n","input_hidden_size = 256\n","target_embedding_size = 256\n","target_hidden_size = 256\n","batch_size = 8\n","\n","# Create data objects\n","enforced_unk_train_dataloader_v3 = data.DataLoader(enforced_unk_train_dataset_v3, batch_size=batch_size, shuffle=True)\n","enforced_unk_validation_dataloader_v3 = data.DataLoader(enforced_unk_validation_dataset_v3, batch_size=batch_size, shuffle=True)\n","input_vocab_size = len(input_token2id)\n","target_vocab_size = len(target_token2id)\n","\n","# Create models\n","enforced_unk_encoder_v3 = SimpleHearthstoneEncoder(input_vocab_size, input_embedding_size, input_hidden_size).to(device)\n","enforced_unk_decoder_v3 = SimpleHearthstoneDecoder(target_vocab_size, target_embedding_size, target_hidden_size, 2 * input_hidden_size, input_max_seq_len).to(device)\n","enforced_unk_generator_v3 = Generator(target_hidden_size, target_vocab_size)\n","enforced_unk_encoder_decoder_v3 = SimpleHearthstoneEncoderDecoder(enforced_unk_encoder_v3, enforced_unk_decoder_v3, enforced_unk_generator_v3.to(device))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PbBZbNtvYhvB","outputId":"9706dbfd-9f85-44d4-ff76-482370a90d22"},"source":["# Train model\n","epochs = 20\n","lr = 1e-3\n","\n","train(enforced_unk_encoder_decoder_v3, enforced_unk_train_dataloader_v3, enforced_unk_validation_dataloader_v3, epochs, lr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","100%|██████████| 60/60 [00:31<00:00,  1.89it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 92.90155431578977\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.81it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 70.86083590341445\n","Validation perplexity: 70.860836\n","Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.90it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 47.77642286643535\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.66it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 54.9881297865004\n","Validation perplexity: 54.988130\n","Epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.90it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 36.58463477760536\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.74it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 46.88114481707129\n","Validation perplexity: 46.881145\n","Epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.90it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 30.313693006299705\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.70it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 39.78882473213219\n","Validation perplexity: 39.788825\n","Epoch 4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.89it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 23.486893803743\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.79it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 30.59235461348468\n","Validation perplexity: 30.592355\n","Epoch 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.90it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 17.381082393832063\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.74it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 22.398059777594625\n","Validation perplexity: 22.398060\n","Epoch 6\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.89it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 12.217228434710618\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.62it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 15.917244522912503\n","Validation perplexity: 15.917245\n","Epoch 7\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.90it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 8.697692718890112\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.68it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 12.321492994599591\n","Validation perplexity: 12.321493\n","Epoch 8\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.89it/s]\n"," 14%|█▍        | 1/7 [00:00<00:00,  6.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 6.640472151253793\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.93it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 9.42590698210788\n","Validation perplexity: 9.425907\n","Epoch 9\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.89it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 5.311904868396282\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.75it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 8.10437417005206\n","Validation perplexity: 8.104374\n","Epoch 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.90it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.495995793279119\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.65it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 7.056398593629289\n","Validation perplexity: 7.056399\n","Epoch 11\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.88it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.020671981885799\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.73it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 6.462063131907395\n","Validation perplexity: 6.462063\n","Epoch 12\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.89it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 3.6450487368526563\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.84it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 5.997051400354593\n","Validation perplexity: 5.997051\n","Epoch 13\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.90it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.84it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 3.3660196887087332\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.77it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 5.830583713406175\n","Validation perplexity: 5.830584\n","Epoch 14\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.91it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.17it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 3.134064816442468\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.54it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 5.398652191580625\n","Validation perplexity: 5.398652\n","Epoch 15\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.91it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.9368403540203487\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.72it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 5.109109934093419\n","Validation perplexity: 5.109110\n","Epoch 16\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.90it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.7602158116509057\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.64it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.879119976635583\n","Validation perplexity: 4.879120\n","Epoch 17\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.90it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.57it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.600889293316594\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.65it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.640098611702336\n","Validation perplexity: 4.640099\n","Epoch 18\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.90it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.471539292243573\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.78it/s]\n","  0%|          | 0/60 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.52993085945324\n","Validation perplexity: 4.529931\n","Epoch 19\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 60/60 [00:31<00:00,  1.89it/s]\n"," 14%|█▍        | 1/7 [00:00<00:01,  5.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.3470666426455615\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:01<00:00,  5.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.324327860800476\n","Validation perplexity: 4.324328\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[70.86083590341445,\n"," 54.9881297865004,\n"," 46.88114481707129,\n"," 39.78882473213219,\n"," 30.59235461348468,\n"," 22.398059777594625,\n"," 15.917244522912503,\n"," 12.321492994599591,\n"," 9.42590698210788,\n"," 8.10437417005206,\n"," 7.056398593629289,\n"," 6.462063131907395,\n"," 5.997051400354593,\n"," 5.830583713406175,\n"," 5.398652191580625,\n"," 5.109109934093419,\n"," 4.879119976635583,\n"," 4.640098611702336,\n"," 4.52993085945324,\n"," 4.324327860800476]"]},"metadata":{"tags":[]},"execution_count":153}]},{"cell_type":"code","metadata":{"id":"Dbv_d4G6ZKY_"},"source":["# Set to True if you want to save this model\n","save_model = False\n","if save_model:\n","    torch.save(enforced_unk_encoder_decoder_v3.state_dict(), os.path.join(project_dir, \"enforced_unk_encoder_decoder_v3.pt\"))\n","\n","# Set to True if you want to load a previously trained model\n","load_model = False\n","if load_model:\n","    enforced_unk_encoder_decoder_v3.load_state_dict(torch.load(os.path.join(project_dir, \"enforced_unk_encoder_decoder_v3.pt\"), map_location=device))\n","    enforced_unk_encoder_decoder_v3.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R07U15Ccbbg8","outputId":"c92772be-376d-4fbc-bdd7-5abcf814c549"},"source":["# spot_check_greedy(enforced_unk_encoder_decoder_v3, simple_test_dataset)\n","spot_check_beam(enforced_unk_encoder_decoder_v3, simple_test_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["===============================\n","Input: UNKUNKUNKNAME_END4ATK_END7DEF_END6COST_END-1DUR_ENDMinionTYPE_ENDNeutralPLAYER_CLS_ENDNILRACE_ENDCommonRARITY_END<b>SpellDamage+1</b>\n","Expected:\n","\n","\tclassUNKUNKUNK(MinionCard):§def__init__(self):§super().__init__(\"UNKUNKUNK\",6,CHARACTER_CLASS.ALL,CARD_RARITY.COMMON)§§defcreate_minion(self,player):§returnMinion(4,7,spell_damage=1)§\n","\n","-got-\n","\n","\tclassUNK(MinionCard):§def__init__(self):§super().__init__(\"Druidofthe\",3,CHARACTER_CLASS.ALL,CARD_RARITY.COMMON,minion_type=MINION_TYPE.MECH)§§defcreate_minion(self,player):§returnMinion(2,3,effects=[Effect(TurnEnded(),ActionTag(Give(ChangeAttack(1)),SelfSelector()))])§\n","===============================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P6IJPRgHb5-N","outputId":"993598d9-12d6-4631-bae0-e2a25f242f5c"},"source":["# Beam Search\n","enforced_unk_v3_beam_matches = evaluate_accuracy(enforced_unk_encoder_decoder_v3, simple_test_dataset, beam_search_decoding, PAD_ID)\n","enforced_unk_v3_beam_bleus = evaluate_bleu(enforced_unk_encoder_decoder_v3, simple_test_dataset, beam_search_decoding, target_id2token, PAD_ID)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 0/66 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","100%|██████████| 66/66 [03:06<00:00,  2.83s/it]\n","100%|██████████| 66/66 [03:12<00:00,  2.92s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aP7L0Vmnb4h-","outputId":"593c39af-8505-4256-a6cb-f260c84a3691"},"source":["# Greedy Search\n","enforced_unk_v3_greedy_matches = evaluate_accuracy(enforced_unk_encoder_decoder_v3, simple_test_dataset, greedy_decoding, PAD_ID)\n","enforced_unk_v3_greedy_bleus = evaluate_bleu(enforced_unk_encoder_decoder_v3, simple_test_dataset, greedy_decoding, target_id2token, PAD_ID)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 0/66 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","100%|██████████| 66/66 [00:04<00:00, 14.64it/s]\n","100%|██████████| 66/66 [00:04<00:00, 14.22it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GtSRKb4gcDp3","outputId":"b8016eda-4e6b-4ebd-eb9f-8d94fc36d098"},"source":["print(\"Metrics for Enforced UNK Encoder/Decoder v3\")\n","print(f\"Beam Accuracy: {sum(enforced_unk_v3_beam_matches) / len(enforced_unk_v3_beam_matches)}\")\n","print(f\"Beam BLEU: {sum(enforced_unk_v3_beam_bleus) / len(enforced_unk_v3_beam_bleus)}\\n\")\n","print(f\"Greedy Accuracy: {sum(enforced_unk_v3_greedy_matches) / len(enforced_unk_v3_greedy_matches)}\")\n","print(f\"Greedy BLEU: {sum(enforced_unk_v3_greedy_bleus) / len(enforced_unk_v3_greedy_bleus)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Metrics for Enforced UNK Encoder/Decoder v3\n","Beam Accuracy: 0.0\n","Beam BLEU: 43.46861853141471\n","\n","Greedy Accuracy: 0.0\n","Greedy BLEU: 43.06489261164009\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sz5DgPIS0MDA"},"source":["# C2W Representations\n","We experiment with an alternative approach to word representations, using [C2W](https://arxiv.org/pdf/1508.02096.pdf)"]},{"cell_type":"code","metadata":{"id":"nj4fumph12q4","executionInfo":{"status":"ok","timestamp":1621302973414,"user_tz":240,"elapsed":415,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["chars = \"!\\\"#$%&'’()*+,-./:;<=>?@[\\\\]^_`{|}~§ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\\n \"\n","char2id = { c: id for (id, c) in enumerate(chars) }\n","char2id[\"PAD\"] = len(char2id)\n","# char2id[\"SOS\"] = len(char2id)\n","# char2id[\"EOS\"] = len(char2id)\n","id2char = { id: c for (c, id) in char2id.items() }"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"06VUdyRg2_EF","executionInfo":{"status":"ok","timestamp":1621302976128,"user_tz":240,"elapsed":794,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["def read_file(path):\n","    \"\"\"Reads file and returns a list of each line\"\"\"\n","    with open(path) as f:\n","        return f.readlines()\n","\n","def get_card_specs_from_lines(lines):\n","    return [card_spec_from_line(line) for line in lines]\n","\n","def tokenize_by_space(lines):\n","    tokenized = []\n","    for line in lines:\n","        tokenized.append(line.split())\n","    return tokenized\n","\n","def tokenize_by_char(token_lists, char2id, max_token_len=None, max_line_len=None):\n","    \"\"\"\n","    :param token_lists: list of lists where each line contains all the tokens in one line\n","    :param char2id: mapping from character to ID to encode with\n","    :return: 3 tensors\n","            - (num_lines, max_line_length, max_token_length) containing all the encoded words and lines\n","            - (num_lines, max_line_length) containing the lengths of each token\n","            - (num_lines,) containing all the lengths of each line\n","    \"\"\"\n","    # Get max token and line lengths\n","    if max_token_len is None:\n","        max_token_len = max([len(token) for tokens in token_lists for token in tokens])\n","    if max_line_len is None:\n","        max_line_len = max([len(tokens) for tokens in token_lists]) + 2\n","    # Create padding token\n","    line_pad = [char2id[\"PAD\"]] * max_token_len\n","#     sos = [char2id[\"SOS\"]] * max_token_len\n","#     eos = [char2id[\"EOS\"]] * max_token_len\n","    total_token_lens = []\n","    total_line_lens = []\n","    total_encoded = []\n","    for tokens in token_lists:\n","        token_lens = []\n","        encoded = []\n","        for token in tokens:\n","            token_lens.append(len(token))\n","            # Pad each encoded token to reach max size\n","            encoded.append([char2id[c] for c in token] + [char2id[\"PAD\"]] * (max_token_len - len(token)))\n","        # Pad token lengths entry to reach max line length\n","        total_token_lens.append([1] + token_lens + [1] + [1] * (max_line_len - (len(tokens) + 2)))\n","        total_line_lens.append(len(tokens) + 2) # 2 for the start and end paddings\n","        # Pad encoded line to reach max line length\n","#         total_encoded.append([sos] + encoded + [eos] + [line_pad] * (max_line_len - (len(tokens) + 2)))\n","        total_encoded.append([line_pad] + encoded + [line_pad] + [line_pad] * (max_line_len - (len(tokens) + 2)))\n","    return torch.LongTensor(total_encoded).to(device), torch.IntTensor(total_token_lens).to(device), torch.IntTensor(total_line_lens).to(device)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"zVnVkb_G0LTK","executionInfo":{"status":"ok","timestamp":1621302977800,"user_tz":240,"elapsed":528,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["class C2W(nn.Module):\n","    def __init__(self, embedding_size, hidden_size, output_size, num_chars):\n","        super(C2W, self).__init__()\n","\n","        self.embedding = nn.Embedding(num_chars, embedding_size)\n","        self.lstm = nn.LSTM(embedding_size, hidden_size, 1, \n","                            batch_first=True, bidirectional=True)\n","        self.output = nn.Linear(2 * hidden_size, output_size)\n","        self.output_size = output_size\n","        \n","    def forward(self, seq, token_lens):\n","        \"\"\"\n","        :param seq: tensor (line_len, max_token_len) containing sequence\n","        :param token_lens: tensor (line_len,) containing token lengths of each word\n","        :returns: tensor (1, line_len, output_size)\n","        \"\"\"\n","        embedded = self.embedding(seq) # (line_len, max_token_len, embedding_size)\n","        packed_embedded = pack_padded_sequence(embedded, token_lens, batch_first=True, enforce_sorted=False)\n","        _, (hidden, _) = self.lstm(packed_embedded) # (2 * num_layers, line_len, hidden_size)\n","        forward_hidden = hidden[::2]\n","        backward_hidden = hidden[1::2]\n","        combined = torch.cat([forward_hidden, backward_hidden], dim=2) # (num_layers, line_len, 2 * hidden_size)\n","        \n","        return self.output(combined[-1:]) # (1, line_len, output_size)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eb2MAxWJBrhA"},"source":["## C2W Encoder\n","We will experiment using an ecoder that uses C2W to encode words. This will differ in that the input will be tokenized by word instead of using BPE and the decoder will generate characters."]},{"cell_type":"code","metadata":{"id":"1BwsSOkSxLp6","executionInfo":{"status":"ok","timestamp":1621302979429,"user_tz":240,"elapsed":583,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["class HearthstoneDatasetC2W(data.Dataset):\n","    def __init__(self, raw_input_lines, raw_target_lines, char2id, input_max_token_len, input_max_seq_len, target_max_token_len, target_max_seq_len):\n","        assert len(raw_input_lines) == len(raw_target_lines)\n","        self.raw_input_lines = raw_input_lines\n","        self.raw_target_lines = raw_target_lines\n","\n","        self.input_words, self.input_token_lengths, self.input_line_lengths = tokenize_by_char(tokenize_by_space(raw_input_lines), char2id, max_token_len=input_max_token_len, max_line_len=input_max_seq_len)\n","        self.target_words, self.target_token_lengths, self.target_line_lengths = tokenize_by_char([list(line) for line in raw_target_lines], char2id, max_token_len=target_max_token_len, max_line_len=target_max_seq_len)\n","\n","    def __len__(self):\n","        return len(self.input_words)\n","\n","    def __getitem__(self, idx):\n","        input_line_length = self.input_line_lengths[idx]\n","        inputs = self.input_words[idx]\n","        input_lengths = self.input_token_lengths[idx]\n","\n","        target_line_length = self.target_line_lengths[idx]\n","        targets = self.target_words[idx]\n","        target_lengths = self.target_token_lengths[idx]\n","\n","        return inputs.to(device), targets.to(device).squeeze(1), input_lengths.to(\"cpu\"), target_lengths.to(\"cpu\")"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"_FhuYPutG--F","executionInfo":{"status":"ok","timestamp":1621302980969,"user_tz":240,"elapsed":715,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["class C2WHearthstoneEncoder(nn.Module):\n","    \"\"\"Encoder for hearthstone tokens using C2W\"\"\"\n","    def __init__(self, c2w, hidden_size, num_layers=3, dropout=0.1):\n","        super(C2WHearthstoneEncoder, self).__init__()\n","        self.c2w = c2w\n","        self.rnn = nn.GRU(\n","            input_size=c2w.output_size,\n","            hidden_size=hidden_size,\n","            num_layers=3,\n","            dropout=dropout,\n","            batch_first=True,\n","            bidirectional=True\n","        )\n","\n","    def forward(self, inputs, token_lens):\n","        \"\"\"\n","        :param inputs: 3d tensor of shape (1, line_len, max_token_len) with the word embeddings for a single line (batch size is 1)\n","        :param token_lens: tensor (1, line_len) containing token lengths of each word\n","\n","        :return: (outputs, hidden) where outputs is 3d tensor of shape (1, max_seq_length, hidden_size)\n","                and hidden is 3d tensor of shape (num_layers, 1, 2*hidden_size)\n","        \"\"\"\n","        inputs = inputs.squeeze(0)\n","        token_lens = token_lens.squeeze(0).to(\"cpu\")\n","        embedded_inputs = self.c2w(inputs, token_lens)\n","\n","        outputs, hidden = self.rnn(embedded_inputs)\n","\n","        forward_hidden = hidden[::2]\n","        backward_hidden = hidden[1::2]\n","        hidden = torch.cat([forward_hidden, backward_hidden], dim=2)\n","\n","        return outputs, hidden"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"ASEkQe_QxD6k"},"source":["raw_input_lines = read_file(train_input_path)\n","raw_target_lines = read_file(train_target_path)\n","input_max_token_len = max([len(word) for line in raw_input_lines for word in line.split()])\n","input_max_seq_len = max([len(line.split()) for line in raw_input_lines]) + 2\n","target_max_seq_len = max([len(list(line)) for line in raw_target_lines]) + 2\n","\n","validation_ratio = 0.1\n","train_size = int((1 - validation_ratio) * len(raw_input_lines))\n","val_size = int(validation_ratio * len(raw_input_lines))\n","c2w_training_dataset = HearthstoneDatasetC2W(raw_input_lines[:train_size], raw_target_lines[:train_size], char2id, input_max_token_len, input_max_seq_len, 1, target_max_seq_len)\n","c2w_validation_dataset = HearthstoneDatasetC2W(raw_input_lines[train_size:], raw_target_lines[train_size:], char2id, input_max_token_len, input_max_seq_len, 1, target_max_seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pN3leqJm32OJ"},"source":["c2w_char_embed_size = 100\n","c2w_hidden_size = 300\n","c2w_output_size = 300\n","input_hidden_size = 300\n","num_chars = len(char2id)\n","\n","target_embedding_size = 100\n","target_hidden_size = 300\n","\n","# Create data objects\n","c2w_training_loader = data.DataLoader(c2w_training_dataset, batch_size=1, shuffle=True)\n","c2w_validation_loader = data.DataLoader(c2w_validation_dataset, batch_size=1, shuffle=True)\n","\n","# Create models\n","c2w = C2W(c2w_char_embed_size, c2w_hidden_size, c2w_output_size, num_chars).to(device)\n","c2w_encoder = C2WHearthstoneEncoder(c2w, input_hidden_size).to(device)\n","c2w_decoder = SimpleHearthstoneDecoder(num_chars, target_embedding_size, target_hidden_size, 2 * input_hidden_size, input_max_seq_len).to(device)\n","c2w_generator = Generator(target_hidden_size, num_chars)\n","c2w_encoder_decoder = SimpleHearthstoneEncoderDecoder(c2w_encoder, c2w_decoder, c2w_generator).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mn-JVMwCZBp"},"source":["def run_epoch_c2w(data_loader, model, loss_compute):\n","    \"\"\"Standard Training and Logging Function\"\"\"\n","    total_tokens = 0\n","    total_loss = 0\n","\n","    for i, (src_ids_BxT, trg_ids_BxL, src_lengths_B, trg_lengths_B) in enumerate(tqdm(data_loader, position=0, leave=True)):\n","        # We define some notations here to help you understand the loaded tensor\n","        # shapes:\n","        #     `B`: batch size\n","        #     `T`: max sequence length of source sentences\n","        #     `L`: max sequence length of target sentences; due to our preprocessing\n","        #        in the beginning, `L` == `T` == 50\n","        # An example of `src_ids_BxT` (when B = 2):\n","        #     [[2, 4, 6, 7, ..., 4, 3, 0, 0, 0],\n","        #    [2, 8, 6, 5, ..., 9, 5, 4, 3, 0]]\n","        # The corresponding `src_lengths_B` would be [47, 49].\n","\n","        src_ids_BxT = src_ids_BxT.to(device)\n","        src_lengths_B = src_lengths_B.to(device)\n","        trg_ids_BxL = trg_ids_BxL.to(device)\n","\n","        del trg_lengths_B     # unused\n","\n","        output, _ = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n","\n","        loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:],\n","                            norm=src_ids_BxT.size(0))\n","        \n","        if i % 20 == 0:\n","            print(f\"Iteration {i} Loss: {loss}\")\n","        total_loss += loss\n","        total_tokens += (trg_ids_BxL[:, 1:] != char2id[\"PAD\"]).data.sum().item() + 1\n","\n","    print(f\"Total loss: {math.exp(total_loss / float(total_tokens))}\")\n","\n","    return math.exp(total_loss / float(total_tokens))\n","\n","def train_c2w(model, train_data_loader, val_data_loader, num_epochs, learning_rate):\n","    # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n","    # computing the loss.\n","    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=char2id[\"PAD\"])\n","    optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Keep track of dev ppl for each epoch.\n","    dev_ppls = []\n","\n","    for epoch in range(num_epochs):\n","        print(\"Epoch\", epoch)\n","\n","        model.train()\n","        train_ppl = run_epoch_c2w(data_loader=train_data_loader, model=model,\n","                                loss_compute=SimpleLossCompute(model.generator,\n","                                                             criterion, optim))\n","\n","        model.eval()\n","        with torch.no_grad():        \n","            dev_ppl = run_epoch_c2w(data_loader=val_data_loader, model=model,\n","                                loss_compute=SimpleLossCompute(model.generator,\n","                                                             criterion, None))\n","            print(\"Validation perplexity: %f\" % dev_ppl)\n","            dev_ppls.append(dev_ppl)\n","        \n","    return dev_ppls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"K3yU2vIv2d27","outputId":"0491c958-c5ee-467e-934f-d8b233c75cb0"},"source":["# Train model\n","epochs = 1\n","lr = 1e-3\n","\n","train_c2w(c2w_encoder_decoder, c2w_training_loader, c2w_validation_loader, epochs, lr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/479 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["<ipython-input-9-e79693af7427>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attention_weights = F.softmax(attention_raw)\n","  0%|          | 1/479 [00:03<27:20,  3.43s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 1004.1509399414062\n"],"name":"stdout"},{"output_type":"stream","text":["  4%|▍         | 21/479 [01:10<26:02,  3.41s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 20 Loss: 1014.3773193359375\n"],"name":"stdout"},{"output_type":"stream","text":["  9%|▊         | 41/479 [02:18<24:19,  3.33s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 40 Loss: 746.728759765625\n"],"name":"stdout"},{"output_type":"stream","text":[" 13%|█▎        | 61/479 [03:25<23:36,  3.39s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 60 Loss: 861.34228515625\n"],"name":"stdout"},{"output_type":"stream","text":[" 17%|█▋        | 81/479 [04:32<22:24,  3.38s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 80 Loss: 826.9150390625\n"],"name":"stdout"},{"output_type":"stream","text":[" 21%|██        | 101/479 [05:40<21:04,  3.34s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 100 Loss: 478.9886779785156\n"],"name":"stdout"},{"output_type":"stream","text":[" 25%|██▌       | 121/479 [06:40<15:25,  2.58s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 120 Loss: 489.9083557128906\n"],"name":"stdout"},{"output_type":"stream","text":[" 29%|██▉       | 141/479 [07:38<15:41,  2.78s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 140 Loss: 1254.7677001953125\n"],"name":"stdout"},{"output_type":"stream","text":[" 34%|███▎      | 161/479 [08:34<15:48,  2.98s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 160 Loss: 521.7062377929688\n"],"name":"stdout"},{"output_type":"stream","text":[" 38%|███▊      | 181/479 [09:27<12:07,  2.44s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 180 Loss: 245.38674926757812\n"],"name":"stdout"},{"output_type":"stream","text":[" 42%|████▏     | 201/479 [10:21<13:20,  2.88s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 200 Loss: 577.2071533203125\n"],"name":"stdout"},{"output_type":"stream","text":[" 46%|████▌     | 221/479 [11:21<13:21,  3.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 220 Loss: 725.4498291015625\n"],"name":"stdout"},{"output_type":"stream","text":[" 50%|█████     | 241/479 [12:24<12:06,  3.05s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 240 Loss: 382.6413879394531\n"],"name":"stdout"},{"output_type":"stream","text":[" 54%|█████▍    | 261/479 [13:26<11:34,  3.18s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 260 Loss: 205.62701416015625\n"],"name":"stdout"},{"output_type":"stream","text":[" 59%|█████▊    | 281/479 [14:29<10:21,  3.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 280 Loss: 525.160400390625\n"],"name":"stdout"},{"output_type":"stream","text":[" 63%|██████▎   | 301/479 [15:33<09:15,  3.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 300 Loss: 701.9708251953125\n"],"name":"stdout"},{"output_type":"stream","text":[" 67%|██████▋   | 321/479 [16:36<08:04,  3.07s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 320 Loss: 1247.2027587890625\n"],"name":"stdout"},{"output_type":"stream","text":[" 71%|███████   | 341/479 [17:32<05:55,  2.57s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 340 Loss: 182.8584747314453\n"],"name":"stdout"},{"output_type":"stream","text":[" 75%|███████▌  | 361/479 [18:21<05:33,  2.83s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 360 Loss: 201.04940795898438\n"],"name":"stdout"},{"output_type":"stream","text":[" 80%|███████▉  | 381/479 [19:20<04:46,  2.93s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 380 Loss: 137.69503784179688\n"],"name":"stdout"},{"output_type":"stream","text":[" 84%|████████▎ | 401/479 [20:20<03:41,  2.84s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 400 Loss: 873.7568359375\n"],"name":"stdout"},{"output_type":"stream","text":[" 88%|████████▊ | 421/479 [21:06<02:18,  2.38s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 420 Loss: 292.6247863769531\n"],"name":"stdout"},{"output_type":"stream","text":[" 92%|█████████▏| 441/479 [21:57<01:46,  2.81s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 440 Loss: 134.40345764160156\n"],"name":"stdout"},{"output_type":"stream","text":[" 96%|█████████▌| 461/479 [22:48<00:42,  2.38s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 460 Loss: 109.41586303710938\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 479/479 [23:34<00:00,  2.95s/it]\n","  0%|          | 0/54 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 4.369875180479339\n"],"name":"stdout"},{"output_type":"stream","text":["\r  2%|▏         | 1/54 [00:00<00:31,  1.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 171.69740295410156\n"],"name":"stdout"},{"output_type":"stream","text":[" 39%|███▉      | 21/54 [00:13<00:22,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 20 Loss: 585.6018676757812\n"],"name":"stdout"},{"output_type":"stream","text":[" 76%|███████▌  | 41/54 [00:25<00:07,  1.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 40 Loss: 210.47496032714844\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 54/54 [00:32<00:00,  1.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.0583144041299906\n","Validation perplexity: 2.058314\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[2.0583144041299906]"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"id":"Hi1bNRk5LuT_","outputId":"9d297e98-fb87-4d86-b91f-a0ee9acf4566"},"source":["# Train for another epoch\n","train_c2w(c2w_encoder_decoder, c2w_training_loader, c2w_validation_loader, epochs, lr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/479 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["<ipython-input-9-e79693af7427>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attention_weights = F.softmax(attention_raw)\n","  0%|          | 1/479 [00:03<25:07,  3.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 104.9936752319336\n"],"name":"stdout"},{"output_type":"stream","text":["  4%|▍         | 21/479 [01:05<23:38,  3.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 20 Loss: 234.01702880859375\n"],"name":"stdout"},{"output_type":"stream","text":["  9%|▊         | 41/479 [02:07<23:16,  3.19s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 40 Loss: 143.1583709716797\n"],"name":"stdout"},{"output_type":"stream","text":[" 13%|█▎        | 61/479 [03:10<22:10,  3.18s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 60 Loss: 279.5332336425781\n"],"name":"stdout"},{"output_type":"stream","text":[" 17%|█▋        | 81/479 [04:12<20:19,  3.07s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 80 Loss: 139.3435821533203\n"],"name":"stdout"},{"output_type":"stream","text":[" 21%|██        | 101/479 [05:15<19:42,  3.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 100 Loss: 232.7386474609375\n"],"name":"stdout"},{"output_type":"stream","text":[" 25%|██▌       | 121/479 [06:18<18:48,  3.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 120 Loss: 255.1192626953125\n"],"name":"stdout"},{"output_type":"stream","text":[" 29%|██▉       | 141/479 [07:18<15:51,  2.82s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 140 Loss: 114.7650375366211\n"],"name":"stdout"},{"output_type":"stream","text":[" 34%|███▎      | 161/479 [08:16<15:04,  2.84s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 160 Loss: 89.224609375\n"],"name":"stdout"},{"output_type":"stream","text":[" 38%|███▊      | 181/479 [09:15<15:27,  3.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 180 Loss: 183.22389221191406\n"],"name":"stdout"},{"output_type":"stream","text":[" 42%|████▏     | 201/479 [10:18<14:30,  3.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 200 Loss: 173.48146057128906\n"],"name":"stdout"},{"output_type":"stream","text":[" 46%|████▌     | 221/479 [11:15<12:23,  2.88s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 220 Loss: 94.85865020751953\n"],"name":"stdout"},{"output_type":"stream","text":[" 50%|█████     | 241/479 [12:17<12:18,  3.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 240 Loss: 170.0771942138672\n"],"name":"stdout"},{"output_type":"stream","text":[" 54%|█████▍    | 261/479 [13:14<09:24,  2.59s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 260 Loss: 142.26699829101562\n"],"name":"stdout"},{"output_type":"stream","text":[" 59%|█████▊    | 281/479 [14:11<10:12,  3.09s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 280 Loss: 57.66069793701172\n"],"name":"stdout"},{"output_type":"stream","text":[" 63%|██████▎   | 301/479 [15:13<09:30,  3.21s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 300 Loss: 230.29859924316406\n"],"name":"stdout"},{"output_type":"stream","text":[" 67%|██████▋   | 321/479 [16:11<08:13,  3.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 320 Loss: 190.509521484375\n"],"name":"stdout"},{"output_type":"stream","text":[" 71%|███████   | 341/479 [17:14<07:17,  3.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 340 Loss: 72.6450424194336\n"],"name":"stdout"},{"output_type":"stream","text":[" 75%|███████▌  | 361/479 [18:19<06:30,  3.31s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 360 Loss: 184.3108367919922\n"],"name":"stdout"},{"output_type":"stream","text":[" 80%|███████▉  | 381/479 [19:22<05:06,  3.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 380 Loss: 163.19735717773438\n"],"name":"stdout"},{"output_type":"stream","text":[" 84%|████████▎ | 401/479 [20:24<04:03,  3.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 400 Loss: 151.927734375\n"],"name":"stdout"},{"output_type":"stream","text":[" 88%|████████▊ | 421/479 [21:27<03:02,  3.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 420 Loss: 206.09600830078125\n"],"name":"stdout"},{"output_type":"stream","text":[" 92%|█████████▏| 441/479 [22:29<02:02,  3.22s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 440 Loss: 159.31797790527344\n"],"name":"stdout"},{"output_type":"stream","text":[" 96%|█████████▌| 461/479 [23:33<00:58,  3.24s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 460 Loss: 68.4783935546875\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 479/479 [24:31<00:00,  3.07s/it]\n","  0%|          | 0/54 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 1.7333565065245211\n"],"name":"stdout"},{"output_type":"stream","text":["\r  2%|▏         | 1/54 [00:01<00:53,  1.01s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 400.7561950683594\n"],"name":"stdout"},{"output_type":"stream","text":[" 39%|███▉      | 21/54 [00:18<00:28,  1.18it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 20 Loss: 176.44232177734375\n"],"name":"stdout"},{"output_type":"stream","text":[" 76%|███████▌  | 41/54 [00:30<00:09,  1.44it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 40 Loss: 416.2169189453125\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 54/54 [00:39<00:00,  1.35it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 1.7130133513195525\n","Validation perplexity: 1.713013\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[1.7130133513195525]"]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"code","metadata":{"id":"8lHJzpA4LuT_","outputId":"9c3127b7-99c7-43da-a2d6-7009456e0c6f"},"source":["# Train for another epoch\n","train_c2w(c2w_encoder_decoder, c2w_training_loader, c2w_validation_loader, epochs, lr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/479 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["<ipython-input-9-e79693af7427>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attention_weights = F.softmax(attention_raw)\n","  0%|          | 1/479 [00:03<25:49,  3.24s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 118.39209747314453\n"],"name":"stdout"},{"output_type":"stream","text":["  4%|▍         | 21/479 [00:59<23:37,  3.09s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 20 Loss: 120.62686157226562\n"],"name":"stdout"},{"output_type":"stream","text":["  9%|▊         | 41/479 [02:04<23:20,  3.20s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 40 Loss: 127.58795166015625\n"],"name":"stdout"},{"output_type":"stream","text":[" 13%|█▎        | 61/479 [03:09<22:13,  3.19s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 60 Loss: 90.86248779296875\n"],"name":"stdout"},{"output_type":"stream","text":[" 17%|█▋        | 81/479 [04:12<20:50,  3.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 80 Loss: 197.1774444580078\n"],"name":"stdout"},{"output_type":"stream","text":[" 21%|██        | 101/479 [05:16<21:00,  3.33s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 100 Loss: 118.23661041259766\n"],"name":"stdout"},{"output_type":"stream","text":[" 25%|██▌       | 121/479 [06:19<19:29,  3.27s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 120 Loss: 173.5605010986328\n"],"name":"stdout"},{"output_type":"stream","text":[" 29%|██▉       | 141/479 [07:25<18:35,  3.30s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 140 Loss: 89.86589813232422\n"],"name":"stdout"},{"output_type":"stream","text":[" 34%|███▎      | 161/479 [08:32<17:19,  3.27s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 160 Loss: 126.35882568359375\n"],"name":"stdout"},{"output_type":"stream","text":[" 38%|███▊      | 181/479 [09:34<15:37,  3.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 180 Loss: 1107.270263671875\n"],"name":"stdout"},{"output_type":"stream","text":[" 42%|████▏     | 201/479 [10:37<14:29,  3.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 200 Loss: 83.2724609375\n"],"name":"stdout"},{"output_type":"stream","text":[" 46%|████▌     | 221/479 [11:40<13:35,  3.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 220 Loss: 151.109130859375\n"],"name":"stdout"},{"output_type":"stream","text":[" 50%|█████     | 241/479 [12:43<12:35,  3.18s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 240 Loss: 224.6641845703125\n"],"name":"stdout"},{"output_type":"stream","text":[" 54%|█████▍    | 261/479 [13:36<07:16,  2.00s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 260 Loss: 147.69778442382812\n"],"name":"stdout"},{"output_type":"stream","text":[" 59%|█████▊    | 281/479 [14:37<10:25,  3.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 280 Loss: 69.943115234375\n"],"name":"stdout"},{"output_type":"stream","text":[" 63%|██████▎   | 301/479 [15:37<08:00,  2.70s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 300 Loss: 92.1741943359375\n"],"name":"stdout"},{"output_type":"stream","text":[" 67%|██████▋   | 321/479 [16:36<08:27,  3.21s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 320 Loss: 113.05504608154297\n"],"name":"stdout"},{"output_type":"stream","text":[" 71%|███████   | 341/479 [17:39<07:15,  3.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 340 Loss: 108.65818786621094\n"],"name":"stdout"},{"output_type":"stream","text":[" 75%|███████▌  | 361/479 [18:37<05:06,  2.60s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 360 Loss: 134.15457153320312\n"],"name":"stdout"},{"output_type":"stream","text":[" 80%|███████▉  | 381/479 [19:27<04:30,  2.76s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 380 Loss: 120.38597106933594\n"],"name":"stdout"},{"output_type":"stream","text":[" 84%|████████▎ | 401/479 [20:12<03:11,  2.46s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 400 Loss: 143.81491088867188\n"],"name":"stdout"},{"output_type":"stream","text":[" 88%|████████▊ | 421/479 [21:04<02:16,  2.35s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 420 Loss: 117.31340789794922\n"],"name":"stdout"},{"output_type":"stream","text":[" 92%|█████████▏| 441/479 [21:53<01:35,  2.52s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 440 Loss: 95.00065612792969\n"],"name":"stdout"},{"output_type":"stream","text":[" 96%|█████████▌| 461/479 [22:44<00:53,  2.97s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 460 Loss: 137.13197326660156\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 479/479 [23:24<00:00,  2.93s/it]\n","  0%|          | 0/54 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 1.5379131258143661\n"],"name":"stdout"},{"output_type":"stream","text":["\r  2%|▏         | 1/54 [00:00<00:29,  1.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 147.22579956054688\n"],"name":"stdout"},{"output_type":"stream","text":[" 39%|███▉      | 21/54 [00:11<00:18,  1.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 20 Loss: 212.70162963867188\n"],"name":"stdout"},{"output_type":"stream","text":[" 76%|███████▌  | 41/54 [00:22<00:07,  1.78it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 40 Loss: 51.27867126464844\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 54/54 [00:30<00:00,  1.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 1.613265676804163\n","Validation perplexity: 1.613266\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[1.613265676804163]"]},"metadata":{"tags":[]},"execution_count":140}]},{"cell_type":"code","metadata":{"id":"d5069ghELuT_","outputId":"244bebd0-fe95-4425-82da-42702bd5f585"},"source":["# Train for another epoch\n","train_c2w(c2w_encoder_decoder, c2w_training_loader, c2w_validation_loader, epochs, lr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/479 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["<ipython-input-14-e79693af7427>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attention_weights = F.softmax(attention_raw)\n","  0%|          | 1/479 [00:02<23:52,  3.00s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 176.2230224609375\n"],"name":"stdout"},{"output_type":"stream","text":["  4%|▍         | 21/479 [00:46<14:07,  1.85s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 20 Loss: 407.892333984375\n"],"name":"stdout"},{"output_type":"stream","text":["  9%|▊         | 41/479 [01:22<12:52,  1.76s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 40 Loss: 131.88021850585938\n"],"name":"stdout"},{"output_type":"stream","text":[" 13%|█▎        | 61/479 [02:18<21:26,  3.08s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 60 Loss: 157.58567810058594\n"],"name":"stdout"},{"output_type":"stream","text":[" 17%|█▋        | 81/479 [03:06<12:19,  1.86s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 80 Loss: 337.029541015625\n"],"name":"stdout"},{"output_type":"stream","text":[" 21%|██        | 101/479 [03:54<18:41,  2.97s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 100 Loss: 142.92381286621094\n"],"name":"stdout"},{"output_type":"stream","text":[" 25%|██▌       | 121/479 [04:55<18:06,  3.04s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 120 Loss: 75.4714584350586\n"],"name":"stdout"},{"output_type":"stream","text":[" 29%|██▉       | 141/479 [05:56<17:24,  3.09s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 140 Loss: 354.0461120605469\n"],"name":"stdout"},{"output_type":"stream","text":[" 34%|███▎      | 161/479 [06:57<16:04,  3.03s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 160 Loss: 109.08141326904297\n"],"name":"stdout"},{"output_type":"stream","text":[" 38%|███▊      | 181/479 [07:58<15:23,  3.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 180 Loss: 131.9510955810547\n"],"name":"stdout"},{"output_type":"stream","text":[" 42%|████▏     | 201/479 [08:59<13:53,  3.00s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 200 Loss: 170.85128784179688\n"],"name":"stdout"},{"output_type":"stream","text":[" 46%|████▌     | 221/479 [09:59<13:16,  3.09s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 220 Loss: 145.7288360595703\n"],"name":"stdout"},{"output_type":"stream","text":[" 50%|█████     | 241/479 [11:00<11:58,  3.02s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 240 Loss: 470.9762878417969\n"],"name":"stdout"},{"output_type":"stream","text":[" 54%|█████▍    | 261/479 [12:02<11:21,  3.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 260 Loss: 99.40811157226562\n"],"name":"stdout"},{"output_type":"stream","text":[" 59%|█████▊    | 281/479 [13:03<09:55,  3.01s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 280 Loss: 91.61802673339844\n"],"name":"stdout"},{"output_type":"stream","text":[" 63%|██████▎   | 301/479 [14:04<09:11,  3.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 300 Loss: 69.38383483886719\n"],"name":"stdout"},{"output_type":"stream","text":[" 67%|██████▋   | 321/479 [15:02<08:07,  3.08s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 320 Loss: 122.82331085205078\n"],"name":"stdout"},{"output_type":"stream","text":[" 71%|███████   | 341/479 [15:59<06:55,  3.01s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 340 Loss: 111.33098602294922\n"],"name":"stdout"},{"output_type":"stream","text":[" 75%|███████▌  | 361/479 [17:00<05:56,  3.02s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 360 Loss: 509.963623046875\n"],"name":"stdout"},{"output_type":"stream","text":[" 80%|███████▉  | 381/479 [18:02<05:09,  3.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 380 Loss: 278.86407470703125\n"],"name":"stdout"},{"output_type":"stream","text":[" 84%|████████▎ | 401/479 [19:00<02:59,  2.31s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 400 Loss: 278.9239196777344\n"],"name":"stdout"},{"output_type":"stream","text":[" 88%|████████▊ | 421/479 [20:05<03:07,  3.24s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 420 Loss: 142.5436553955078\n"],"name":"stdout"},{"output_type":"stream","text":[" 92%|█████████▏| 441/479 [21:07<02:00,  3.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 440 Loss: 198.1768798828125\n"],"name":"stdout"},{"output_type":"stream","text":[" 96%|█████████▌| 461/479 [22:10<00:55,  3.08s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 460 Loss: 167.74264526367188\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 479/479 [23:07<00:00,  2.90s/it]\n","  0%|          | 0/54 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 1.7783313433274683\n"],"name":"stdout"},{"output_type":"stream","text":["\r  2%|▏         | 1/54 [00:00<00:30,  1.72it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 238.40606689453125\n"],"name":"stdout"},{"output_type":"stream","text":[" 39%|███▉      | 21/54 [00:19<00:35,  1.06s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 20 Loss: 373.0147399902344\n"],"name":"stdout"},{"output_type":"stream","text":[" 76%|███████▌  | 41/54 [00:41<00:14,  1.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 40 Loss: 174.46994018554688\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 54/54 [00:51<00:00,  1.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 1.8271525224257794\n","Validation perplexity: 1.827153\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[1.8271525224257794]"]},"metadata":{"tags":[]},"execution_count":108}]},{"cell_type":"code","metadata":{"id":"5kZbHOESLuT_","outputId":"b2a07d74-d796-43fb-d358-8bdcdde98ebc"},"source":["# Train for another epoch\n","train_c2w(c2w_encoder_decoder, c2w_training_loader, c2w_validation_loader, epochs, lr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/479 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["<ipython-input-14-e79693af7427>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attention_weights = F.softmax(attention_raw)\n","  0%|          | 1/479 [00:03<25:01,  3.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 264.27606201171875\n"],"name":"stdout"},{"output_type":"stream","text":["  4%|▍         | 21/479 [01:08<24:12,  3.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 20 Loss: 178.02565002441406\n"],"name":"stdout"},{"output_type":"stream","text":["  9%|▊         | 41/479 [02:14<24:44,  3.39s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 40 Loss: 82.88799285888672\n"],"name":"stdout"},{"output_type":"stream","text":[" 13%|█▎        | 61/479 [03:19<22:00,  3.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 60 Loss: 163.76466369628906\n"],"name":"stdout"},{"output_type":"stream","text":[" 17%|█▋        | 81/479 [04:25<21:36,  3.26s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 80 Loss: 279.50128173828125\n"],"name":"stdout"},{"output_type":"stream","text":[" 21%|██        | 101/479 [05:29<19:53,  3.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 100 Loss: 268.5705871582031\n"],"name":"stdout"},{"output_type":"stream","text":[" 25%|██▌       | 121/479 [06:34<19:32,  3.27s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 120 Loss: 165.120361328125\n"],"name":"stdout"},{"output_type":"stream","text":[" 29%|██▉       | 141/479 [07:37<17:15,  3.06s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 140 Loss: 175.19920349121094\n"],"name":"stdout"},{"output_type":"stream","text":[" 34%|███▎      | 161/479 [08:40<16:44,  3.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 160 Loss: 480.13031005859375\n"],"name":"stdout"},{"output_type":"stream","text":[" 38%|███▊      | 181/479 [09:42<15:37,  3.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 180 Loss: 129.5928497314453\n"],"name":"stdout"},{"output_type":"stream","text":[" 42%|████▏     | 201/479 [10:45<14:41,  3.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 200 Loss: 248.69854736328125\n"],"name":"stdout"},{"output_type":"stream","text":[" 46%|████▌     | 221/479 [11:49<13:48,  3.21s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 220 Loss: 171.96432495117188\n"],"name":"stdout"},{"output_type":"stream","text":[" 50%|█████     | 241/479 [12:51<12:29,  3.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 240 Loss: 125.17884063720703\n"],"name":"stdout"},{"output_type":"stream","text":[" 54%|█████▍    | 261/479 [13:53<11:37,  3.20s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 260 Loss: 171.51480102539062\n"],"name":"stdout"},{"output_type":"stream","text":[" 59%|█████▊    | 281/479 [14:56<10:16,  3.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 280 Loss: 82.65476989746094\n"],"name":"stdout"},{"output_type":"stream","text":[" 63%|██████▎   | 301/479 [15:59<09:29,  3.20s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 300 Loss: 99.76980590820312\n"],"name":"stdout"},{"output_type":"stream","text":[" 67%|██████▋   | 321/479 [17:05<09:03,  3.44s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 320 Loss: 162.74586486816406\n"],"name":"stdout"},{"output_type":"stream","text":[" 71%|███████   | 341/479 [18:07<07:28,  3.25s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 340 Loss: 167.47909545898438\n"],"name":"stdout"},{"output_type":"stream","text":[" 75%|███████▌  | 361/479 [19:12<06:17,  3.20s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 360 Loss: 190.69244384765625\n"],"name":"stdout"},{"output_type":"stream","text":[" 80%|███████▉  | 381/479 [20:15<04:48,  2.94s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 380 Loss: 234.4292449951172\n"],"name":"stdout"},{"output_type":"stream","text":[" 84%|████████▎ | 401/479 [21:19<04:01,  3.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 400 Loss: 114.69639587402344\n"],"name":"stdout"},{"output_type":"stream","text":[" 88%|████████▊ | 421/479 [22:23<03:05,  3.19s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 420 Loss: 233.8186492919922\n"],"name":"stdout"},{"output_type":"stream","text":[" 92%|█████████▏| 441/479 [23:25<01:57,  3.09s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 440 Loss: 162.4336395263672\n"],"name":"stdout"},{"output_type":"stream","text":[" 96%|█████████▌| 461/479 [24:29<00:57,  3.21s/it]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 460 Loss: 92.80748748779297\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 479/479 [25:26<00:00,  3.19s/it]\n","  0%|          | 0/54 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 1.7841438718447378\n"],"name":"stdout"},{"output_type":"stream","text":["\r  2%|▏         | 1/54 [00:00<00:30,  1.72it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 174.72935485839844\n"],"name":"stdout"},{"output_type":"stream","text":[" 39%|███▉      | 21/54 [00:15<00:25,  1.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 20 Loss: 277.00164794921875\n"],"name":"stdout"},{"output_type":"stream","text":[" 76%|███████▌  | 41/54 [00:29<00:10,  1.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 40 Loss: 280.65606689453125\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 54/54 [00:38<00:00,  1.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 1.8372679519248165\n","Validation perplexity: 1.837268\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[1.8372679519248165]"]},"metadata":{"tags":[]},"execution_count":125}]},{"cell_type":"code","metadata":{"id":"AAePHRl1LuUA"},"source":["# Set to True if you want to save this model\n","save_model = False\n","if save_model:\n","    torch.save(c2w_encoder_decoder.state_dict(), os.path.join(project_dir, \"c2w_encoder_decoder_drop_3e.pt\"))\n","\n","# Set to True if you want to load a previously trained model\n","load_model = True\n","if load_model:\n","    c2w_encoder_decoder.load_state_dict(torch.load(os.path.join(project_dir, \"c2w_encoder_decoder_3e.pt\"), map_location=device))\n","    c2w_encoder_decoder.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJr_0_JQJ94r"},"source":["## C2W Decoding"]},{"cell_type":"code","metadata":{"id":"HBBTQY7vLuUA"},"source":["# Read and tokenize test inputs and targets\n","test_raw_inputs = read_file(test_input_path)\n","test_raw_targets = read_file(test_target_path)\n","\n","# Truncate line tokens so it matches training data\n","trunc_test_inputs = []\n","for line in test_raw_inputs:\n","    tokens = line.split()\n","    if len(tokens) > input_max_seq_len - 2:\n","        trunc_test_inputs.append(\" \".join(tokens[:input_max_seq_len - 2]))\n","    else:\n","        trunc_test_inputs.append(line)\n","        \n","trunc_test_raw_targets = []\n","for line in test_raw_targets:\n","    if len(line) > target_max_seq_len - 2:\n","        trunc_test_raw_targets.append(line[:target_max_seq_len - 2])\n","    else:\n","        trunc_test_raw_targets.append(line)\n","\n","c2w_test_dataset = HearthstoneDatasetC2W(trunc_test_inputs, trunc_test_raw_targets, char2id, input_max_token_len, input_max_seq_len, 1, target_max_seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nPq335j_KBjh"},"source":["def greedy_decoding_c2w(model, src_ids, src_lengths, max_len):\n","    \"\"\"Greedily decode a sentence for EncoderDecoder. Make sure to chop off the \n","         EOS token!\"\"\"\n","\n","    with torch.no_grad():\n","        encoder_outputs, encoder_hidden = model.encode(src_ids, src_lengths)\n","#         prev_y = torch.ones(1, 1).fill_(char2id[\"SOS\"]).type_as(src_ids)\n","        prev_y = torch.ones(1, 1).fill_(char2id[\"PAD\"]).type_as(src_ids)\n","    \n","    output = []\n","    hidden = None\n","\n","    for i in range(max_len):\n","        with torch.no_grad():\n","            outputs, hidden = model.decode(prev_y, encoder_outputs, encoder_hidden, hidden)\n","            prob = model.generator(outputs[:, -1])\n","        d, next_word = torch.max(prob, dim=1)\n","        next_word = next_word.data.item()\n","        output.append(next_word)\n","        prev_y = torch.ones(1, 1).type_as(src_ids).fill_(next_word)\n","\n","    output = np.array(output)\n","\n","    # Cut off everything starting from </s>.\n","#     first_pad = np.where(output[1:] == char2id[\"EOS\"])[0]\n","    first_pad = np.where(output[1:] == char2id[\"PAD\"])[0]\n","    if len(first_pad) > 0:\n","        output = output[:first_pad[0]]\n","\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_7lgzOpLuUA"},"source":["def spot_check_greedy_c2w(model, dataset, idx=None, n=1):\n","    \"\"\"Compare a (random) generated and target sequence using greedy search\"\"\"\n","    for i in range(n):\n","        if idx is None:\n","            idx = np.random.randint(0, len(dataset))\n","        inp_ids, trg_ids, inp_lens, trg_lens = dataset[idx: idx+1]\n","        greedy_decoded = greedy_decoding_c2w(model, inp_ids, inp_lens, target_max_seq_len)\n","        inp_ids = inp_ids[0][1:]\n","        trg_ids = trg_ids[0][1:]\n","        stripped_trg_ids = trg_ids[trg_ids != char2id[\"PAD\"]].tolist()[:-1]\n","        stripped_inp_ids = inp_ids[inp_ids != char2id[\"PAD\"]].tolist()\n","#         stripped_inp_ids = inp_ids[inp_ids != char2id[\"PAD\"]]\n","#         stripped_inp_ids = stripped_inp_ids[stripped_inp_ids != char2id[\"EOS\"]].tolist()\n","        print(\"===============================\")\n","        print(f\"Input: {tokens_to_text(stripped_inp_ids, id2char)}\")\n","        print(f\"Expected:\\n\\n\\t{tokens_to_text(stripped_trg_ids, id2char)}\\n\\n-got-\\n\\n\\t{tokens_to_text(greedy_decoded, id2char)}\")\n","        print(\"===============================\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8aODx6PtLuUA","outputId":"e3bf1208-966f-4ac9-b6cd-800f8a3ce1cc"},"source":["spot_check_greedy_c2w(c2w_encoder_decoder, c2w_test_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<ipython-input-262-e79693af7427>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attention_weights = F.softmax(attention_raw)\n"],"name":"stderr"},{"output_type":"stream","text":["===============================\n","Input: ConcealNAME_END-1ATK_END-1DEF_END1COST_END-1DUR_ENDSpellTYPE_ENDRoguePLAYER_CLS_ENDNILRACE_ENDCommonRARITY_ENDGiveyourminions<b>Stealth</b>untilyournextturn.\n","Expected:\n","\n","\tclass Conceal(SpellCard):§    def __init__(self):§        super().__init__(\"Conceal\", 1, CHARACTER_CLASS.ROGUE, CARD_RARITY.COMMON)§§    def use(self, player, game):§        super().use(player, game)§        for minion in player.minions:§            if not minion.stealth:§                minion.add_buff(BuffUntil(Stealth(), TurnStarted()))§\n","\n","-got-\n","\n","\tclass Sindind(MinionCard):§    def __init__(self):§        super().__init__(\"Sring Cring\", 1, CHARACTER_CLASS.ALL, CARD_RARITY.COMMON, minion_type=MINION_TYPE.MECH)§§    def create_minion(self, player):§        return Minion(2, 5)§\n","===============================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JAbX9rSdLuUB"},"source":["def beam_search_decoding_c2w(model, src_ids, src_lengths, max_len, k=25):\n","    \"\"\"Keep expanding top k most likely sequences\"\"\"\n","    with torch.no_grad():\n","        encoder_outputs, encoder_hidden = model.encode(src_ids, src_lengths)\n","    \n","    # Keep track of top outputs stores as (log prob, output ID seq, hidden)\n","    top_outputs = [(0, [char2id[\"SOS\"]], None)]\n","\n","    for i in range(max_len):\n","        new_top_outputs = []\n","        for log_prob, output, hidden in top_outputs:\n","            # Get last token of candidate output sequence and use as input to decoder\n","            prev_y = torch.ones(1, 1).type_as(src_ids).fill_(output[-1])\n","            probs = None\n","            h = None\n","            with torch.no_grad():\n","                o, h = model.decode(prev_y, encoder_outputs, encoder_hidden, hidden)\n","                probs = model.generator(o[:, -1])\n","            # Get top k log probs and ids\n","            topk_log_probs, topk_ids = torch.topk(probs,k, dim=1)\n","            for token_log_prob, token_id in zip(topk_log_probs[0], topk_ids[0]):\n","                new_top_outputs.append((log_prob + token_log_prob.data.item(), output + [token_id.data.item()], h))\n","        # Get top k most likely output sequences up to this point\n","        new_top_outputs = sorted(new_top_outputs, key=lambda d: d[0], reverse=True)\n","        top_outputs = new_top_outputs[:k]\n","    \n","    # Get the most likely output sequence of all top outputs\n","    output = np.array(max(top_outputs, key=lambda d: d[0])[1])\n","\n","    # Cut off everything starting from </s>.\n","    first_pad = np.where(output[1:] == char2id[\"EOS\"])[0]\n","    if len(first_pad) > 0:\n","        output = output[:first_pad[0]]\n","\n","    return output[1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5tCLXRNXLuUB"},"source":["def spot_check_beam_c2w(model, dataset, idx=None, n=1):\n","    \"\"\"Compare a (random) generated and target sequence using greedy search\"\"\"\n","    for i in range(n):\n","        if idx is None:\n","            idx = np.random.randint(0, len(dataset))\n","        inp_ids, trg_ids, inp_lens, trg_lens = dataset[idx: idx+1]\n","        greedy_decoded = beam_search_decoding_c2w(model, inp_ids, inp_lens, target_max_seq_len)\n","        inp_ids = inp_ids[0][1:]\n","        trg_ids = trg_ids[0][1:]\n","        stripped_trg_ids = trg_ids[trg_ids != char2id[\"PAD\"]].tolist()[:-1]\n","        stripped_inp_ids = inp_ids[inp_ids != char2id[\"PAD\"]]\n","        stripped_inp_ids = stripped_inp_ids[stripped_inp_ids != char2id[\"EOS\"]].tolist()\n","        print(\"===============================\")\n","        print(f\"Input: {tokens_to_text(stripped_inp_ids, id2char)}\")\n","        print(f\"Expected:\\n\\n\\t{tokens_to_text(stripped_trg_ids, id2char)}\\n\\n-got-\\n\\n\\t{tokens_to_text(greedy_decoded, id2char)}\")\n","        print(\"===============================\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Dd5GxoSLuUB","outputId":"67ef3754-6fae-47c7-82e2-d305adc407ca"},"source":["spot_check_beam_c2w(c2w_encoder_decoder, c2w_test_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<ipython-input-9-e79693af7427>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attention_weights = F.softmax(attention_raw)\n"],"name":"stderr"},{"output_type":"stream","text":["===============================\n","Input: ManaWraithNAME_END2ATK_END2DEF_END2COST_END-1DUR_ENDMinionTYPE_ENDNeutralPLAYER_CLS_ENDNILRACE_ENDRareRARITY_ENDALLminionscost(1)more.\n","Expected:\n","\n","\tclass ManaWraith(MinionCard):§    def __init__(self):§        super().__init__(\"Mana Wraith\", 2, CHARACTER_CLASS.ALL, CARD_RARITY.RARE)§§    def create_minion(self, player):§        return Minion(2, 2, auras=[Aura(ManaChange(1), CardSelector(BothPlayer(), IsMinion()))])§\n","\n","\n","-got-\n","\n","\tclass SpellCard(MinionCard):§    def __init__(self):§        super().__init__(\"Ancient\", 1, CHARACTER_CLASS.ALL, CARD_RARITY.COMMON, minion_type=MINION_TYPE.BEAST)§§    def create_minion(self, player):§        return Minion(1, 1, effects=[Effect(TurnEnded(player=BothPlayer()), ActionTag(Give([Buff(ChangeAttack(2)), SelfSelector()))])§\n","===============================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9v171RYSLuUB"},"source":["## Evaluate"]},{"cell_type":"code","metadata":{"id":"rPRI3qWuLuUB"},"source":["def evaluate_accuracy_c2w(model, test_dataset, decoder, pad_token):\n","    \"\"\"\n","    :param model: model to evaluate\n","    :param test_dataset: test dataset to evaluate that yields (input, target_tokens, input_length (or empty value), target_length (or empty value))\n","            target_tokens should have sequence that starts and ends with SOS and EOS tokens respectively and may be padded with pad_token\n","    :param decoder: decoder to evaluate with; returns a list of predicted tokens with SOS, EOS, and PAD tokens removed\n","    :param pad_token: padding token used in target_tokens\n","    \"\"\"\n","    matches = []\n","    for i in tqdm(range(len(test_dataset)), position=0, leave=True):\n","        inp, trg_tokens, inp_len, trg_len = test_dataset[i: i+1]\n","        trunc_trg_tokens = trg_tokens[0][trg_tokens[0] != pad_token].tolist()\n","\n","        pred_tokens = decoder(model, inp, inp_len, target_max_seq_len)\n","        \n","        matches.append(1 if pred_tokens == trg_tokens else 0)\n","    return matches"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aw31YseILuUB"},"source":["def evaluate_bleu_c2w(model, test_dataset, decoder, token2str, pad_token):\n","    \"\"\"\n","    :param model: model to evaluate\n","    :param test_dataset: test dataset to evaluate that yields (input, target_tokens, input_length (or empty value), target_length (or empty value))\n","            target_tokens should have sequence that starts and ends with SOS and EOS tokens respectively and may be padded with pad_token\n","    :param decoder: decoder to evaluate with; returns a list of predicted tokens with SOS, EOS, and PAD tokens removed\n","    :param token2str: mapping from token to string\n","    :param pad_token: padding token used in target_tokens\n","    \"\"\"\n","    bleu_scores = []\n","    for i in tqdm(range(len(test_dataset)), position=0, leave=True):\n","        inp, trg_tokens, inp_len, trg_len = test_dataset[i: i+1]\n","        trunc_trg_tokens = trg_tokens[0][trg_tokens[0] != pad_token].tolist()\n","\n","        pred_tokens = decoder(model, inp, inp_len, target_max_seq_len)\n","        pred_text = \"\".join([id2char[t] for t in pred_tokens])\n","        trg_text = \"\".join([id2char[t] for t in trunc_trg_tokens])\n","\n","        bleu_scores.append(sacrebleu.raw_corpus_bleu([pred_text], [[trg_text]], 0.01).score)\n","\n","    return bleu_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sKjZl3RoLuUC"},"source":["# Beam Search\n","# c2w_beam_matches = evaluate_accuracy_c2w(c2w_encoder_decoder, c2w_test_dataset, beam_search_decoding_c2w, char2id[\"PAD\"])\n","# c2w_beam_bleus = evaluate_bleu_c2w(c2w_encoder_decoder, c2w_test_dataset, beam_search_decoding_c2w, id2char, char2id[\"PAD\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvSvxxSoLuUC","outputId":"d47c42e7-5efc-4a09-9ac6-1e03caea4cf4"},"source":["# Greedy Search\n","c2w_greedy_matches = evaluate_accuracy_c2w(c2w_encoder_decoder, c2w_test_dataset, greedy_decoding_c2w, char2id[\"PAD\"])\n","c2w_greedy_bleus = evaluate_bleu_c2w(c2w_encoder_decoder, c2w_test_dataset, greedy_decoding_c2w, id2char, char2id[\"PAD\"])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 0/66 [00:00<?, ?it/s]<ipython-input-262-e79693af7427>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attention_weights = F.softmax(attention_raw)\n","100%|██████████| 66/66 [01:03<00:00,  1.05it/s]\n","100%|██████████| 66/66 [01:03<00:00,  1.04it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"6sh0jpu_LuUC","outputId":"3d630790-5d8a-4ebe-8f00-0fbbce20561c"},"source":["print(\"Metrics for C2W Encoder / Decoder\")\n","# print(f\"Beam Accuracy: {sum(c2w_beam_matches) / len(c2w_beam_matches)}\")\n","# print(f\"Beam BLEU: {sum(c2w_beam_bleus) / len(c2w_beam_bleus)}\\n\")\n","print(f\"Greedy Accuracy: {sum(c2w_greedy_matches) / len(c2w_greedy_matches)}\")\n","print(f\"Greedy BLEU: {sum(c2w_greedy_bleus) / len(c2w_greedy_bleus)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Metrics for C2W Encoder / Decoder\n","Greedy Accuracy: 0.0\n","Greedy BLEU: 13.058270175547005\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0ImyfouuGXkY"},"source":["# Separate Fields\n","We previously treated our input sequence as one long string. We experiment with separating each field in the card spec to see if we can apply different attention weights to each."]},{"cell_type":"code","metadata":{"id":"ERnvm4aiMw1G","executionInfo":{"status":"ok","timestamp":1621303020736,"user_tz":240,"elapsed":810,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["class HearthstoneCardSpec(object):\n","    def __init__(self, name, attack, defense, cost, durability, card_type, player_cls, race, rarity, description):\n","        self.name = name\n","        self.attack = attack\n","        self.defense = defense\n","        self.cost = cost\n","        self.durability = durability\n","        self.card_type = card_type\n","        self.player_cls = player_cls\n","        self.race = race\n","        self.rarity = rarity\n","        self.description = description\n","\n","class HearthstoneCardSpecTokenized(object):\n","    def __init__(self, card_spec, char2id):\n","        self.tokenized_name, self.name_token_lengths, self.name_line_length = tokenize_by_char([card_spec.name.split()], char2id)\n","        self.tokenized_name = self.tokenized_name.squeeze(0)[1:-1]\n","        self.name_token_lengths = self.name_token_lengths.squeeze(0).to(\"cpu\")[1:-1]\n","        self.name_line_length = self.name_line_length.item() - 2\n","\n","        self.tokenized_attack, self.attack_token_lengths, self.attack_line_length = tokenize_by_char([card_spec.attack.split()], char2id)\n","        self.tokenized_attack = self.tokenized_attack.squeeze(0)[1:-1]\n","        self.attack_token_lengths = self.attack_token_lengths.squeeze(0).to(\"cpu\")[1:-1]\n","        self.attack_line_length = self.attack_line_length.item() - 2\n","\n","        self.tokenized_defense, self.defense_token_lengths, self.defense_line_length = tokenize_by_char([card_spec.defense.split()], char2id)\n","        self.tokenized_defense = self.tokenized_defense.squeeze(0)[1:-1]\n","        self.defense_token_lengths = self.defense_token_lengths.squeeze(0).to(\"cpu\")[1:-1]\n","        self.defense_line_length = self.defense_line_length.item() - 2\n","\n","        self.tokenized_cost, self.cost_token_lengths, self.cost_line_length = tokenize_by_char([card_spec.cost.split()], char2id)\n","        self.tokenized_cost = self.tokenized_cost.squeeze(0)[1:-1]\n","        self.cost_token_lengths = self.cost_token_lengths.squeeze(0).to(\"cpu\")[1:-1]\n","        self.cost_line_length = self.cost_line_length.item() - 2\n","\n","        self.tokenized_durability, self.durability_token_lengths, self.durability_line_length = tokenize_by_char([card_spec.durability.split()], char2id)\n","        self.tokenized_durability = self.tokenized_durability.squeeze(0)[1:-1]\n","        self.durability_token_lengths = self.durability_token_lengths.squeeze(0).to(\"cpu\")[1:-1]\n","        self.durability_line_length = self.durability_line_length.item() - 2\n","\n","        self.tokenized_card_type, self.card_type_token_lengths, self.card_type_line_length = tokenize_by_char([card_spec.card_type.split()], char2id)\n","        self.tokenized_card_type = self.tokenized_card_type.squeeze(0)[1:-1]\n","        self.card_type_token_lengths = self.card_type_token_lengths.squeeze(0).to(\"cpu\")[1:-1]\n","        self.card_type_line_length = self.card_type_line_length.item() - 2\n","\n","        self.tokenized_player_cls, self.player_cls_token_lengths, self.player_cls_line_length = tokenize_by_char([card_spec.player_cls.split()], char2id)\n","        self.tokenized_player_cls = self.tokenized_player_cls.squeeze(0)[1:-1]\n","        self.player_cls_token_lengths = self.player_cls_token_lengths.squeeze(0).to(\"cpu\")[1:-1]\n","        self.player_cls_line_length = self.player_cls_line_length.item() - 2\n","\n","        self.tokenized_race, self.race_token_lengths, self.race_line_length = tokenize_by_char([card_spec.race.split()], char2id)\n","        self.tokenized_race = self.tokenized_race.squeeze(0)[1:-1]\n","        self.race_token_lengths = self.race_token_lengths.squeeze(0).to(\"cpu\")[1:-1]\n","        self.race_line_length = self.race_line_length.item() - 2\n","\n","        self.tokenized_rarity, self.rarity_token_lengths, self.rarity_line_length = tokenize_by_char([card_spec.rarity.split()], char2id)\n","        self.tokenized_rarity = self.tokenized_rarity.squeeze(0)[1:-1]\n","        self.rarity_token_lengths = self.rarity_token_lengths.squeeze(0).to(\"cpu\")[1:-1]\n","        self.rarity_line_length = self.rarity_line_length.item() - 2\n","\n","        self.tokenized_description, self.description_token_lengths, self.description_line_length = tokenize_by_char([card_spec.description.split()], char2id)\n","        self.tokenized_description = self.tokenized_description.squeeze(0)[1:-1]\n","        self.description_token_lengths = self.description_token_lengths.squeeze(0).to(\"cpu\")[1:-1]\n","        self.description_line_length = self.description_line_length.item() - 2\n","\n","def card_spec_from_line(line):\n","    name, rest = line.split(\"NAME_END\")\n","    attack, rest = rest.split(\"ATK_END\")\n","    defense, rest = rest.split(\"DEF_END\")\n","    cost, rest = rest.split(\"COST_END\")\n","    durability, rest = rest.split(\"DUR_END\")\n","    card_type, rest = rest.split(\"TYPE_END\")\n","    player_cls, rest = rest.split(\"PLAYER_CLS_END\")\n","    race, rest = rest.split(\"RACE_END\")\n","    rarity, description = rest.split(\"RARITY_END\")\n","    return HearthstoneCardSpec(name.strip(), attack.strip(), defense.strip(), cost.strip(), durability.strip(), card_type.strip(), player_cls.strip(), race.strip(), rarity.strip(), description.strip())"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"-elhtypOGvoQ","executionInfo":{"status":"ok","timestamp":1621303033379,"user_tz":240,"elapsed":806,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["class HearthstoneDatasetByField(data.Dataset):\n","    def __init__(self, raw_input_lines, raw_target_lines, char2id):\n","        assert len(raw_input_lines) == len(raw_target_lines)\n","        self.raw_input_lines = raw_input_lines\n","        self.raw_target_lines = raw_target_lines\n","\n","        self.hearthstone_card_specs = [card_spec_from_line(line) for line in raw_input_lines]\n","        self.hearthstone_card_specs_encoded = [HearthstoneCardSpecTokenized(card_spec, char2id) for card_spec in self.hearthstone_card_specs]\n","        self.tokenized_targets_ids = [torch.LongTensor([char2id[\"PAD\"]] + [char2id[c] for c in line] + [char2id[\"PAD\"]]).to(device) for line in raw_target_lines]\n","\n","    def __len__(self):\n","        return len(self.raw_input_lines)\n","\n","    def __getitem__(self, idx):\n","        return self.hearthstone_card_specs_encoded[idx], self.tokenized_targets_ids[idx]"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"_qDV4S2SaPch","executionInfo":{"status":"ok","timestamp":1621303046743,"user_tz":240,"elapsed":12935,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["raw_input_lines = read_file(train_input_path)\n","raw_target_lines = read_file(train_target_path)\n","max_target_seq_len = max([len(line) for line in raw_target_lines])\n","validation_ratio = 0.1\n","train_size = int((1 - validation_ratio) * len(raw_input_lines))\n","val_size = int(validation_ratio * len(raw_input_lines))\n","c2c_training_dataset = HearthstoneDatasetByField(raw_input_lines[:train_size], raw_target_lines[:train_size], char2id)\n","c2c_validation_dataset = HearthstoneDatasetByField(raw_input_lines[train_size:], raw_target_lines[train_size:], char2id)"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fR0bDEAwSM5t"},"source":["# Card2Code Model\n","We build a version of the Card2Code to compare performance"]},{"cell_type":"markdown","metadata":{"id":"q3QkyQq9CzAs"},"source":["## Encoder / Attention (section 4)\n","1. Separate each field (e.g. name, description, health...)\n","2. Tokenize each field and get representations using C2W (representations are learned on character level)\n","3. Feed fields with multiple words through a Bi-LSTM to get context aware representation (not sure if this becomes the field representation or it's concatenated to the tokens)\n","4. Apply linear layers on all token representations to map to same dimension\n","5. Compute scalar attention coefficients $a_{ki}$ for token $x_{ki}$ by solving $a_{ki} = softmax(v(f(x_{ki}), h_{t-1}))$\n","\t- $f$ is the mapping function of the linear layers in 4\n","\t- $v$ is a function that concatenates $f(x_{ki})$ and $h_{t-1}$ then feeds it through linear -> tanh -> linear layers\n","\t- $h_{t-1}$ is the previous state in the RNN\n","6. Compute overall input vector representation as summation of tokens and attention coefficients $z_t = \\sum_{k, i} a_{ki} * f(x_{ki})$\n","7. Compute new RNN hidden state $h_t = g(y_{t-1}, h_{t-1}, z_t)$\n","    - Calculate in decoder\n","\t- $g$ uses an LSTM\n","\t- $y_{t-1}$ is the previous context encoded at character level\n"]},{"cell_type":"code","metadata":{"id":"UlUdA-Zr1WEH","executionInfo":{"status":"ok","timestamp":1621303050362,"user_tz":240,"elapsed":723,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["class HearthstoneEncoder(nn.Module):\n","    \"\"\"\n","    Fields: name, atk, def, cost, dur, type, player cls, race, rarity, description\n","        - name and description are text fields\n","    \"\"\"\n","    def __init__(self, c2w, text_field_hidden_size, output_size, text_field_num_layers=1):\n","        super(HearthstoneEncoder, self).__init__()\n","        self.c2w = c2w\n","        self.text_field_encoder = nn.LSTM(\n","            input_size=c2w.output_size,\n","            hidden_size=text_field_hidden_size,\n","            num_layers=text_field_num_layers,\n","            batch_first=True,\n","            bidirectional=True\n","        )\n","        # Field projection layers\n","        self.singular_field_encoding = nn.Linear(c2w.output_size, output_size)\n","        self.text_field_encoding = nn.Linear(2 * text_field_hidden_size, output_size)\n","        self.output_size = output_size\n","\n","    def encode_singular_field(self, field, token_lengths):\n","        embedding = self.c2w(field, token_lengths) # (num_layers=1, num_tokens_in_field, embedding_size)\n","        return self.singular_field_encoding(embedding.squeeze(0)) # (num_tokens_in_field, output_size)\n","\n","    def encode_text_field(self, field, token_lengths):\n","        embedding = self.c2w(field, token_lengths) # (num_layers=1, num_tokens_in_field, embedding_size)\n","        output, (_, _) = self.text_field_encoder(embedding) # (1, num_tokens_in_field, 2 * hidden_size)\n","        return self.text_field_encoding(output.squeeze(0)) # (num_tokens_in_field, output_size)\n","\n","    def forward(self, card_spec_tokenized):\n","        \"\"\"\n","        :param card_spec_tokenized: HearthstoneCardSpecTokenized object containing the tokenized specs for 1 card\n","        :return: list of length <# fields> containing tensors (num_tokens_in_field, output_size) where each tensor contains\n","                encodings of all the tokens in a single field; fields are ordered as [name, atk, def, cost, dur, type, cls, race, rarity, desc]\n","        \"\"\"\n","        # Encode each field - only name and description are text fields\n","        # All will be of size (num_tokens_in_field, output_size)\n","        encoded_name = self.encode_text_field(card_spec_tokenized.tokenized_name, card_spec_tokenized.name_token_lengths)\n","        encoded_attack = self.encode_singular_field(card_spec_tokenized.tokenized_attack, card_spec_tokenized.attack_token_lengths)\n","        encoded_defense = self.encode_singular_field(card_spec_tokenized.tokenized_defense, card_spec_tokenized.defense_token_lengths)\n","        encoded_cost = self.encode_singular_field(card_spec_tokenized.tokenized_cost, card_spec_tokenized.cost_token_lengths)\n","        encoded_durability = self.encode_singular_field(card_spec_tokenized.tokenized_durability, card_spec_tokenized.durability_token_lengths)\n","        encoded_card_type = self.encode_singular_field(card_spec_tokenized.tokenized_card_type, card_spec_tokenized.card_type_token_lengths)\n","        encoded_player_cls = self.encode_singular_field(card_spec_tokenized.tokenized_player_cls, card_spec_tokenized.player_cls_token_lengths)\n","        encoded_race = self.encode_singular_field(card_spec_tokenized.tokenized_race, card_spec_tokenized.race_token_lengths)\n","        encoded_rarity = self.encode_singular_field(card_spec_tokenized.tokenized_rarity, card_spec_tokenized.rarity_token_lengths)\n","        encoded_description = self.encode_text_field(card_spec_tokenized.tokenized_description, card_spec_tokenized.description_token_lengths)\n","        all_encoded_fields = [encoded_name, encoded_attack, encoded_defense, encoded_cost, encoded_durability, encoded_card_type, encoded_player_cls, encoded_race, encoded_rarity, encoded_description]\n","        \n","        return all_encoded_fields"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"f3LHGo2EO1eg","executionInfo":{"status":"ok","timestamp":1621303052511,"user_tz":240,"elapsed":390,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["class HearthstoneAttention(nn.Module):\n","    def __init__(self, encoding_output_size, decoder_hidden_size):\n","        super(HearthstoneAttention, self).__init__()\n","        self.attention_weights = nn.Linear(encoding_output_size + decoder_hidden_size, 1)\n","        self.decoder_hidden_size = decoder_hidden_size\n","\n","    def forward(self, all_encoded_fields, prev_decoder_hidden_state=None):\n","        \"\"\"\n","        :param all_encoded_fields: list of length <# fields> containing tensors (num_tokens_in_field, encoding_output_size) where each tensor contains\n","                encodings of all the tokens in a single field\n","        :param prev_decoder_hidden: tensor (1, decoder_hidden_size) representing final hidden state of decoder from previous timestep\n","        :return: vector of size (encoding_output_size,) representing attention as a linear combination of all the token representations\n","        \"\"\"\n","        if prev_decoder_hidden_state is None:\n","            prev_decoder_hidden_state = torch.zeros((1, self.decoder_hidden_size)).to(device)\n","        encoded_input_tokens = torch.cat(all_encoded_fields, dim=0) # (total_tokens, encoding_output_size)\n","        concat_encoded_tokens = torch.cat([encoded_input_tokens, prev_decoder_hidden_state.squeeze(0).repeat(encoded_input_tokens.size(0), 1).to(device)], dim=1) # (total_tokens, projection_size + decoder_hidden_size)\n","\n","        # Compute attention weights by applying nonlinearity (tanh) and using linear layer to map to 1 dimension\n","        raw_attention_weights = self.attention_weights(torch.tanh(concat_encoded_tokens)) # (total_tokens, 1)\n","        attention_weights = F.softmax(raw_attention_weights, dim=0) # (total_tokens, 1)\n","        \n","        attention_vector = torch.mul(attention_weights, encoded_input_tokens).sum(dim=0) # (encoding_output_size,)\n","\n","        return attention_vector"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O9AUh4UBi-du","executionInfo":{"status":"ok","timestamp":1621303054436,"user_tz":240,"elapsed":627,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}},"outputId":"19f5db38-16f8-43bc-bce0-c52f6118781c"},"source":["# Params taken from setup section\n","hearthstone_c2w = C2W(100, 300, 300, len(chars) + 1).to(device) # char embd size, hidden size, output size, vocab size\n","hearthstone_encoder = HearthstoneEncoder(hearthstone_c2w, 300, 300).to(device) # c2w, text field hidden size, output size\n","hearthstone_attention = HearthstoneAttention(300, 300).to(device)\n","\n","hearthstone_c2w.eval()\n","hearthstone_encoder.eval()\n","hearthstone_attention.eval()"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["HearthstoneAttention(\n","  (attention_weights): Linear(in_features=600, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"ldO7WtTHUa2Z","executionInfo":{"status":"ok","timestamp":1621303054651,"user_tz":240,"elapsed":631,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["# Sample of how to use models to compute encodings and attention\n","sample_spec_input, sample_target = c2c_training_dataset[0]\n","all_encoded_fields = hearthstone_encoder(sample_spec_input)\n","attention_vector = hearthstone_attention(all_encoded_fields)"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ux_Ddb-iLuUE"},"source":["## Card2Code Encoder With Simple Decoder\n","This differs from card2code by not using pointer network architectures."]},{"cell_type":"code","metadata":{"id":"YATbB3KFLuUE","executionInfo":{"status":"ok","timestamp":1621303058141,"user_tz":240,"elapsed":815,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["class C2CDecoder(nn.Module):\n","    def __init__(self, hidden_size, attention, enc_output_size, vocab_size):\n","        super(C2CDecoder, self).__init__()\n","        # [name, atk, def, cost, dur, type, cls, race, rarity, desc]\n","        self.attention = attention\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.combine_attention = nn.Linear(enc_output_size + hidden_size, hidden_size)\n","        \n","        self.rnn_num_layers = 1\n","        self.hidden_size = hidden_size\n","        self.rnn = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, batch_first=True, num_layers=self.rnn_num_layers, dropout=0.1)\n","        \n","        \n","    def forward_step(self, prev_embed, hidden, all_encoded_fields):\n","        \"\"\"\n","        :param prev_embed: 3d tensor of shape (batch_size, 1, embed_size) containing word embeddings\n","                from previous time step\n","        :param hidden: 3d tensor of shape (num_layers, batch_size, hidden_size) representing current decoder hidden state\n","\n","        :return: [pre_output, hidden] of current time step\n","        \"\"\"\n","        attn = self.attention(all_encoded_fields, hidden[-1])\n","        concat_attn = torch.cat((prev_embed.squeeze(1), attn.unsqueeze(0)), dim=-1)\n","        combined = self.combine_attention(concat_attn).unsqueeze(1)\n","        \n","        return self.rnn(combined, hidden)\n","\n","    def forward(self, inputs, all_encoded_fields, hidden=None, max_output_len=None):\n","        # Initialize values if not given\n","        if max_output_len is None:\n","            max_output_len = inputs.size(1)\n","        if hidden is None:\n","            hidden = torch.zeros((self.rnn_num_layers, inputs.size(0), self.hidden_size)).to(device)\n","            \n","        embedded = self.embedding(inputs)\n","        dropped_embedded = self.dropout(embedded)\n","\n","        # Generate output and hidden for each word\n","        pre_output_vectors = []\n","        for i in range(max_output_len):\n","            prev_embed = dropped_embedded[:, i].unsqueeze(1)\n","            pre_output, hidden = self.forward_step(prev_embed, hidden, all_encoded_fields)\n","            pre_output_vectors.append(pre_output)\n","\n","        outputs = torch.cat(pre_output_vectors, dim=1)\n","        return outputs, hidden"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"FuJIumoWLuUE","executionInfo":{"status":"ok","timestamp":1621303060516,"user_tz":240,"elapsed":747,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["class C2CEncoderDecoder(nn.Module):\n","    def __init__(self, encoder, decoder, generator):\n","        \"\"\"\n","        Inputs:\n","          - `encoder`: an `Encoder` object.\n","          - `decoder`: a `Decoder` object.\n","          - `generator`: a `Generator` object. Essentially a linear mapping. See\n","              the next code cell.\n","        \"\"\"\n","        super(C2CEncoderDecoder, self).__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.generator = generator\n","\n","    def forward(self, card_spec_tokenized, target):\n","        \"\"\"Take in and process masked source and target sequences.\n","        Returns the decoder outputs, see the above cell.\n","        \"\"\"\n","        all_encoded_fields = self.encode(card_spec_tokenized)\n","        return self.decode(target[:,:-1], all_encoded_fields)\n","\n","    def encode(self, card_spec_tokenized):\n","        return self.encoder(card_spec_tokenized)\n","\n","    def decode(self, target, all_encoded_fields, decoder_hidden=None):\n","        return self.decoder(target, all_encoded_fields, hidden=decoder_hidden)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"r_5ReKWPLuUE","executionInfo":{"status":"ok","timestamp":1621303133905,"user_tz":240,"elapsed":409,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["def run_epoch_c2c(dataset, model, loss_compute):\n","    \"\"\"Standard Training and Logging Function\"\"\"\n","    total_tokens = 0\n","    total_loss = 0\n","\n","    for i in tqdm(range(len(dataset)), position=0, leave=True):\n","        card_spec, target = dataset[i]\n","        target = target.unsqueeze(0)\n","        output, _ = model(card_spec, target)\n","        \n","        loss = loss_compute(x=output, y=target[:, 1:],\n","                            norm=target.size(0))\n","        total_loss += loss\n","        total_tokens += target.size(1) - 1\n","        \n","        if i % 100 == 0:\n","            print(f\"Iteration {i} Loss: {loss}\")\n","\n","    print(f\"Total loss: {math.exp(total_loss / float(total_tokens))}\")\n","\n","    return math.exp(total_loss / float(total_tokens))\n","\n","def train_c2c(model, train_dataset, val_dataset, num_epochs, learning_rate):\n","    # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n","    # computing the loss.\n","    criterion = nn.NLLLoss(reduction=\"sum\")\n","    optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Keep track of dev ppl for each epoch.\n","    dev_ppls = []\n","\n","    for epoch in range(num_epochs):\n","        print(\"Epoch\", epoch)\n","\n","        model.train()\n","        train_ppl = run_epoch_c2c(dataset=train_dataset, model=model,\n","                                loss_compute=SimpleLossCompute(model.generator,\n","                                                             criterion, optim))\n","\n","        model.eval()\n","        with torch.no_grad():        \n","            dev_ppl = run_epoch_c2c(dataset=val_dataset, model=model,\n","                                loss_compute=SimpleLossCompute(model.generator,\n","                                                             criterion, None))\n","            print(\"Validation perplexity: %f\" % dev_ppl)\n","            dev_ppls.append(dev_ppl)\n","        \n","    return dev_ppls"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iq7z6Nb1LuUF","executionInfo":{"status":"ok","timestamp":1621303149438,"user_tz":240,"elapsed":389,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}},"outputId":"a8e08f78-c0a9-4027-ad77-f14ed2b6cd94"},"source":["# Set params\n","c2w_char_embed_size = 100\n","c2w_hidden_size = 300\n","c2w_output_size = 300\n","encoder_text_hidden_size = 300\n","encoder_output_size = 300\n","decoder_output_size = 300\n","decoder_hidden_size = 300\n","batch_size = 1\n","\n","# Params taken from setup section\n","c2c_c2w = C2W(c2w_char_embed_size, c2w_hidden_size, c2w_output_size, len(char2id)).to(device) # char embd size, hidden size, output size, vocab size\n","c2c_encoder = HearthstoneEncoder(c2c_c2w, encoder_text_hidden_size, encoder_output_size).to(device) # c2w, text field hidden size, output size\n","c2c_attention = HearthstoneAttention(encoder_output_size, decoder_output_size).to(device)\n","c2c_decoder = C2CDecoder(decoder_hidden_size, c2c_attention, encoder_output_size, len(char2id)).to(device)\n","c2c_generator = Generator(decoder_hidden_size, len(char2id)).to(device)\n","c2c_encoder_decoder = C2CEncoderDecoder(c2c_encoder, c2c_decoder, c2c_generator).to(device)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"-yqefbk0LuUF","executionInfo":{"status":"ok","timestamp":1621303359415,"user_tz":240,"elapsed":208092,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}},"outputId":"13dfa752-32dc-49a6-eda3-2a230dd817e4"},"source":["num_epochs = 1\n","lr = 1e-3\n","\n","train_c2c(c2c_encoder_decoder, c2c_training_dataset, c2c_validation_dataset, num_epochs, lr)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/479 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 1/479 [00:00<02:36,  3.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 1289.3021240234375\n"],"name":"stdout"},{"output_type":"stream","text":[" 21%|██        | 101/479 [00:38<02:16,  2.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 100 Loss: 290.7304382324219\n"],"name":"stdout"},{"output_type":"stream","text":[" 42%|████▏     | 201/479 [01:21<02:12,  2.10it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 200 Loss: 536.466064453125\n"],"name":"stdout"},{"output_type":"stream","text":[" 63%|██████▎   | 301/479 [02:02<01:31,  1.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 300 Loss: 507.6236572265625\n"],"name":"stdout"},{"output_type":"stream","text":[" 84%|████████▎ | 401/479 [02:44<00:27,  2.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 400 Loss: 123.83995056152344\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 479/479 [03:19<00:00,  2.41it/s]\n","  0%|          | 0/54 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.8841519765791883\n"],"name":"stdout"},{"output_type":"stream","text":["  4%|▎         | 2/54 [00:00<00:12,  4.13it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 744.932861328125\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 54/54 [00:08<00:00,  6.53it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 2.012009187257607\n","Validation perplexity: 2.012009\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[2.012009187257607]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2rt9MKX8LuUF","executionInfo":{"status":"ok","timestamp":1621303809555,"user_tz":240,"elapsed":210317,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}},"outputId":"92fe7f74-9bb7-4a26-be81-492d748c9681"},"source":["# Train for 1 more epoch\n","train_c2c(c2c_encoder_decoder, c2c_training_dataset, c2c_validation_dataset, num_epochs, lr)"],"execution_count":48,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/479 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 1/479 [00:00<02:39,  3.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 194.34280395507812\n"],"name":"stdout"},{"output_type":"stream","text":[" 21%|██        | 101/479 [00:38<02:16,  2.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 100 Loss: 126.01023864746094\n"],"name":"stdout"},{"output_type":"stream","text":[" 42%|████▏     | 201/479 [01:22<02:15,  2.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 200 Loss: 334.14080810546875\n"],"name":"stdout"},{"output_type":"stream","text":[" 63%|██████▎   | 301/479 [02:04<01:34,  1.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 300 Loss: 326.5388488769531\n"],"name":"stdout"},{"output_type":"stream","text":[" 84%|████████▎ | 401/479 [02:47<00:27,  2.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 400 Loss: 101.48735809326172\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 479/479 [03:21<00:00,  2.38it/s]\n","  0%|          | 0/54 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 1.735957484606414\n"],"name":"stdout"},{"output_type":"stream","text":["  4%|▎         | 2/54 [00:00<00:12,  4.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 590.7501220703125\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 54/54 [00:08<00:00,  6.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 1.7810753438023887\n","Validation perplexity: 1.781075\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[1.7810753438023887]"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8WL5SPyjQEPu","executionInfo":{"status":"ok","timestamp":1621304246107,"user_tz":240,"elapsed":209766,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}},"outputId":"dcfb6255-30b9-413f-b0a1-ba47d1eef416"},"source":["# Train for 1 more epoch\n","train_c2c(c2c_encoder_decoder, c2c_training_dataset, c2c_validation_dataset, num_epochs, lr)"],"execution_count":59,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/479 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 1/479 [00:00<02:44,  2.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 152.2542724609375\n"],"name":"stdout"},{"output_type":"stream","text":[" 21%|██        | 101/479 [00:38<02:17,  2.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 100 Loss: 96.17437744140625\n"],"name":"stdout"},{"output_type":"stream","text":[" 42%|████▏     | 201/479 [01:22<02:16,  2.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 200 Loss: 274.4189453125\n"],"name":"stdout"},{"output_type":"stream","text":[" 63%|██████▎   | 301/479 [02:03<01:32,  1.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 300 Loss: 255.27357482910156\n"],"name":"stdout"},{"output_type":"stream","text":[" 84%|████████▎ | 401/479 [02:46<00:27,  2.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 400 Loss: 94.45476531982422\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 479/479 [03:20<00:00,  2.38it/s]\n","  0%|          | 0/54 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 1.5817935244476857\n"],"name":"stdout"},{"output_type":"stream","text":["  4%|▎         | 2/54 [00:00<00:12,  4.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 513.6979370117188\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 54/54 [00:08<00:00,  6.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 1.695475679824379\n","Validation perplexity: 1.695476\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[1.695475679824379]"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SRf3_M8vSJwE","executionInfo":{"status":"ok","timestamp":1621304794015,"user_tz":240,"elapsed":210478,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}},"outputId":"a04cf2c9-04ba-4bdd-bc56-aeee65ad4a37"},"source":["# Train for 1 more epoch\n","train_c2c(c2c_encoder_decoder, c2c_training_dataset, c2c_validation_dataset, num_epochs, lr)"],"execution_count":70,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/479 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 1/479 [00:00<02:43,  2.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 125.07328796386719\n"],"name":"stdout"},{"output_type":"stream","text":[" 21%|██        | 101/479 [00:39<02:21,  2.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 100 Loss: 78.52368927001953\n"],"name":"stdout"},{"output_type":"stream","text":[" 42%|████▏     | 201/479 [01:23<02:15,  2.05it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 200 Loss: 227.59451293945312\n"],"name":"stdout"},{"output_type":"stream","text":[" 63%|██████▎   | 301/479 [02:04<01:33,  1.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 300 Loss: 227.42515563964844\n"],"name":"stdout"},{"output_type":"stream","text":[" 84%|████████▎ | 401/479 [02:47<00:28,  2.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 400 Loss: 90.56700897216797\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 479/479 [03:21<00:00,  2.37it/s]\n","  0%|          | 0/54 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 1.5010941351463838\n"],"name":"stdout"},{"output_type":"stream","text":["  4%|▎         | 2/54 [00:00<00:13,  3.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Iteration 0 Loss: 432.57843017578125\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 54/54 [00:08<00:00,  6.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total loss: 1.6648790582181232\n","Validation perplexity: 1.664879\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[1.6648790582181232]"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"5h5kNfjILuUF","executionInfo":{"status":"ok","timestamp":1621305035005,"user_tz":240,"elapsed":790,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["# Set to True if you want to save this model\n","save_model = False\n","if save_model:\n","    torch.save(c2c_encoder_decoder.state_dict(), os.path.join(project_dir, \"5-17 Fork\", \"c2c_encoder_decoder_4e.pt\"))\n","\n","# Set to True if you want to load a previously trained model\n","load_model = False\n","if load_model:\n","    c2c_encoder_decoder.load_state_dict(torch.load(os.path.join(project_dir, \"5-17 Fork\", \"c2c_encoder_decoder_2e.pt\"), map_location=device))\n","    c2c_encoder_decoder.eval()"],"execution_count":77,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5VElPrGGLuUF"},"source":["## Evaluate"]},{"cell_type":"code","metadata":{"id":"3fHoD0vrLuUF","executionInfo":{"status":"ok","timestamp":1621303381879,"user_tz":240,"elapsed":1768,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["# Read and tokenize test inputs and targets\n","test_raw_inputs = read_file(test_input_path)\n","test_raw_targets = read_file(test_target_path)\n","\n","trunc_test_raw_targets = []\n","for line in test_raw_targets:\n","    if len(line) > max_target_seq_len - 2:\n","        trunc_test_raw_targets.append(line[:max_target_seq_len - 2])\n","    else:\n","        trunc_test_raw_targets.append(line)\n","\n","c2c_test_dataset = HearthstoneDatasetByField(test_raw_inputs, trunc_test_raw_targets, char2id)"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"IAd4KTXWLuUF","executionInfo":{"status":"ok","timestamp":1621303381881,"user_tz":240,"elapsed":1383,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["def greedy_decoding_c2c(model, card_spec_tokenized, max_len):\n","    \"\"\"Greedily decode a sentence for EncoderDecoder. Make sure to chop off the \n","         EOS token!\"\"\"\n","\n","    with torch.no_grad():\n","        all_encoded_fields = model.encode(card_spec_tokenized)\n","        prev_y = torch.ones(1, 1).fill_(char2id[\"PAD\"]).long().to(device)\n","    \n","    output = []\n","    hidden = None\n","\n","    for i in range(max_len):\n","        with torch.no_grad():\n","            outputs, hidden = model.decode(prev_y, all_encoded_fields, hidden)\n","            prob = model.generator(outputs[:, -1])\n","        d, next_word = torch.max(prob, dim=1)\n","        next_word = next_word.data.item()\n","        output.append(next_word)\n","        prev_y = torch.ones(1, 1).fill_(next_word).long().to(device)\n","\n","    output = np.array(output)\n","\n","    # Cut off everything starting from </s>.\n","    first_pad = np.where(output == char2id[\"PAD\"])[0]\n","    if len(first_pad) > 0:\n","        output = output[:first_pad[0]]\n","\n","    \n","    return output"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMLgGIatLuUG","executionInfo":{"status":"ok","timestamp":1621303830843,"user_tz":240,"elapsed":384,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["def spot_check_greedy_c2c(model, dataset, idx=None, n=1):\n","    \"\"\"Compare a (random) generated and target sequence using greedy search\"\"\"\n","    for i in range(n):\n","        if idx is None:\n","            idx = np.random.randint(0, len(dataset))\n","        card_spec_tokenized, targets = dataset[idx: idx+1]\n","        greedy_decoded = greedy_decoding_c2c(model, card_spec_tokenized[0], max_target_seq_len)\n","        targets = targets[0][1:-1]\n","        stripped_trg = targets[targets != char2id[\"PAD\"]].tolist()\n","        print(\"===============================\")\n","        print(f\"Expected:\\n\\n\\t{tokens_to_text(stripped_trg, id2char)}\\n\\n-got-\\n\\n\\t{tokens_to_text(greedy_decoded, id2char)}\")\n","        print(\"===============================\")"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WGX-bhggLuUG","executionInfo":{"status":"ok","timestamp":1621305051396,"user_tz":240,"elapsed":1513,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}},"outputId":"fbd08adf-11d7-4c10-8983-eb6ae706c38c"},"source":["spot_check_greedy_c2c(c2c_encoder_decoder, c2c_test_dataset)"],"execution_count":80,"outputs":[{"output_type":"stream","text":["===============================\n","Expected:\n","\n","\tclass DarkscaleHealer(MinionCard):§    def __init__(self):§        super().__init__(\"Darkscale Healer\", 5, CHARACTER_CLASS.ALL, CARD_RARITY.COMMON, battlecry=Battlecry(Heal(2), CharacterSelector()))§§    def create_minion(self, player):§        return Minion(4, 5)§\n","\n","\n","-got-\n","\n","\tclass Stormper(MinionCard):§    def __init__(self):§        super().__init__(\"Starger\", 3, CHARACTER_CLASS.ALL, CARD_RARITY.COMMON, minion_type=MINION_TYPE.BEAST)§§    def create_minion(self, player):§        return Minion(3, 5, deathrattle=Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Deathrattle(Death\n","===============================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N6NQU_rmLuUG"},"source":["def beam_search_decoding_c2c(model, card_spec_tokenized, max_len, k=25):\n","    \"\"\"Keep expanding top k most likely sequences\"\"\"\n","    with torch.no_grad():\n","        all_encoded_fields = model.encode(card_spec_tokenized)\n","    \n","    # Keep track of top outputs stores as (log prob, output ID seq, hidden)\n","    top_outputs = [(0, [char2id[\"SOS\"]], None)]\n","\n","    for i in range(max_len):\n","        new_top_outputs = []\n","        for log_prob, output, hidden in top_outputs:\n","            # Get last token of candidate output sequence and use as input to decoder\n","            prev_y = torch.ones(1, 1).long().fill_(output[-1]).to(device)\n","            probs = None\n","            h = None\n","            with torch.no_grad():\n","                o, h = model.decode(prev_y, all_encoded_fields, hidden)\n","                probs = model.generator(o[:, -1])\n","            # Get top k log probs and ids\n","            topk_log_probs, topk_ids = torch.topk(probs,k, dim=1)\n","            for token_log_prob, token_id in zip(topk_log_probs[0], topk_ids[0]):\n","                new_top_outputs.append((log_prob + token_log_prob.data.item(), output + [token_id.data.item()], h))\n","        # Get top k most likely output sequences up to this point\n","        new_top_outputs = sorted(new_top_outputs, key=lambda d: d[0], reverse=True)\n","        top_outputs = new_top_outputs[:k]\n","    \n","    # Get the most likely output sequence of all top outputs\n","    output = np.array(max(top_outputs, key=lambda d: d[0])[1])\n","\n","    # Cut off everything starting from </s>.\n","    first_pad = np.where(output[1:] == char2id[\"EOS\"])[0]\n","    if len(first_pad) > 0:\n","        output = output[:first_pad[0]]\n","\n","    return output[1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYIsDjLVLuUG"},"source":["def spot_check_beam_c2c(model, dataset, idx=None, n=1):\n","    \"\"\"Compare a (random) generated and target sequence using greedy search\"\"\"\n","    for i in range(n):\n","        if idx is None:\n","            idx = np.random.randint(0, len(dataset))\n","        card_spec_tokenized, targets = dataset[idx: idx+1]\n","        greedy_decoded = beam_search_decoding_c2c(model, card_spec_tokenized[0], max_target_seq_len)\n","        targets = targets[0][1:-1]\n","        stripped_trg = targets[targets != char2id[\"PAD\"]].tolist()\n","        print(\"===============================\")\n","        print(f\"Expected:\\n\\n\\t{tokens_to_text(stripped_trg, id2char)}\\n\\n-got-\\n\\n\\t{tokens_to_text(greedy_decoded, id2char)}\")\n","        print(\"===============================\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"dvD2MdcZLuUG","executionInfo":{"status":"error","timestamp":1621303851322,"user_tz":240,"elapsed":726,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}},"outputId":"32537d12-bf2c-4b68-a2ee-56f7fc566800"},"source":["spot_check_beam_c2c(c2c_encoder_decoder, c2c_test_dataset)"],"execution_count":56,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-2a9c2635b020>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspot_check_beam_c2c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc2c_encoder_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2c_test_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'spot_check_beam_c2c' is not defined"]}]},{"cell_type":"code","metadata":{"id":"siJENl5yLuUG","executionInfo":{"status":"ok","timestamp":1621304389942,"user_tz":240,"elapsed":395,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["def evaluate_accuracy_c2c(model, test_dataset, decoder):\n","    \"\"\"\n","    :param model: model to evaluate\n","    :param test_dataset: test dataset to evaluate that yields (input, target_tokens, input_length (or empty value), target_length (or empty value))\n","            target_tokens should have sequence that starts and ends with SOS and EOS tokens respectively and may be padded with pad_token\n","    :param decoder: decoder to evaluate with; returns a list of predicted tokens with SOS, EOS, and PAD tokens removed\n","    :param pad_token: padding token used in target_tokens\n","    \"\"\"\n","    matches = []\n","    for i in tqdm(range(len(test_dataset)), position=0, leave=True):\n","        card_spec_tokenized, targets = test_dataset[i: i+1]\n","        \n","        targets = targets[0][1:-1]\n","        stripped_trg = targets[targets != char2id[\"PAD\"]].tolist()\n","\n","        pred_tokens = decoder(model, card_spec_tokenized[0], max_target_seq_len).tolist()\n","        \n","        matches.append(1 if pred_tokens == stripped_trg else 0)\n","    return matches"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"8KvdkxegLuUG","executionInfo":{"status":"ok","timestamp":1621304390636,"user_tz":240,"elapsed":809,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}}},"source":["def evaluate_bleu_c2c(model, test_dataset, decoder, token2str):\n","    \"\"\"\n","    :param model: model to evaluate\n","    :param test_dataset: test dataset to evaluate that yields (input, target_tokens, input_length (or empty value), target_length (or empty value))\n","            target_tokens should have sequence that starts and ends with SOS and EOS tokens respectively and may be padded with pad_token\n","    :param decoder: decoder to evaluate with; returns a list of predicted tokens with SOS, EOS, and PAD tokens removed\n","    :param token2str: mapping from token to string\n","    :param pad_token: padding token used in target_tokens\n","    \"\"\"\n","    bleu_scores = []\n","    for i in tqdm(range(len(test_dataset)), position=0, leave=True):\n","        card_spec_tokenized, targets = test_dataset[i: i+1]\n","        targets = targets[0][1:-1]\n","        stripped_trg = targets[targets != char2id[\"PAD\"]].tolist()\n","\n","        pred_tokens = decoder(model, card_spec_tokenized[0], max_target_seq_len)\n","        pred_text = \"\".join([id2char[t] for t in pred_tokens])\n","        trg_text = \"\".join([id2char[t] for t in stripped_trg])\n","\n","        bleu_scores.append(sacrebleu.raw_corpus_bleu([pred_text], [[trg_text]], 0.01).score)\n","\n","    return bleu_scores"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T_bgu3imLuUH","executionInfo":{"status":"ok","timestamp":1621305216466,"user_tz":240,"elapsed":160214,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}},"outputId":"9051362c-81a6-45fe-9de1-d88ab9b462f2"},"source":["# Greedy Search\n","c2c_greedy_matches = evaluate_accuracy_c2c(c2c_encoder_decoder, c2c_test_dataset, greedy_decoding_c2c)\n","c2c_greedy_bleus = evaluate_bleu_c2c(c2c_encoder_decoder, c2c_test_dataset, greedy_decoding_c2c, id2char)"],"execution_count":81,"outputs":[{"output_type":"stream","text":["100%|██████████| 66/66 [01:19<00:00,  1.21s/it]\n","100%|██████████| 66/66 [01:19<00:00,  1.21s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LVizxhXdLuUH","executionInfo":{"status":"ok","timestamp":1621305238950,"user_tz":240,"elapsed":832,"user":{"displayName":"Jason Kung","photoUrl":"","userId":"02313880960468387478"}},"outputId":"93b39695-4d22-44be-b5a4-1278e2645779"},"source":["print(\"Metrics for C2W Encoder / Decoder\")\n","# print(f\"Beam Accuracy: {sum(c2w_beam_matches) / len(c2w_beam_matches)}\")\n","# print(f\"Beam BLEU: {sum(c2w_beam_bleus) / len(c2w_beam_bleus)}\\n\")\n","print(f\"Greedy Accuracy: {sum(c2c_greedy_matches) / len(c2c_greedy_matches)}\")\n","print(f\"Greedy BLEU: {sum(c2c_greedy_bleus) / len(c2c_greedy_bleus)}\")"],"execution_count":83,"outputs":[{"output_type":"stream","text":["Metrics for C2W Encoder / Decoder\n","Greedy Accuracy: 0.0\n","Greedy BLEU: 12.389094297166572\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DFjyTXKWC4lr"},"source":["## Decode (section 5)\n","1. Select a predictor $r_t$ using probabilities from $softmax(h_{t-1}, z_t)$ to generate a sequence $s_t$\n","\t- 1 predictor for each field that copies from the field\n","\t- 1 predictor that generates characters\n","\t- $|x| + 1$ predictors for $|x|$ fields\n","\t- $h_{t-1}$ and $z_t$ are from encode\n","2. If generate char selected, generate the char using probabilities from $softmax(h_t)$\n","3. If generate field is selected for a singular field (one word), copy all the characters from that word with probability 1\n","4. If generate field is selected for a text field (multiple words), copy word from text based on pointer network probability\n","\t- Probability of word c_i is $p(c_i) = softmax(v(h(c_i), q))$\n","\t- $h$ is a representation of word $c_i$\n","        - $f(x_{ki})$ from attention calculation\n","\t- $v$ is a function that concatenates $h(c_i)$ and $q$ then feeds it through linear -> tanh -> linear layers\n","\t- $q$ is a concatenation of $h_{t-1}$ and $z_t$ from encode\n","5. At each time step, generate states $S = (r_t, s_t)$ based on scores $V(S) = \\log P(s_t|y_1...y_{t-1}, x, r_t) + \\log P(r_t | y_1...y_{t-1}, x) + V(prev(S))$\n","\t- Expand top $n$ states\n","\t- All states producing same output up to that point are merged by summing their probabilities"]},{"cell_type":"code","metadata":{"id":"nlkm7rFjZCWf"},"source":["class HearthstoneDecoder(nn.Module):\n","    def __init__(self, embedding_size, hidden_size, enc_output_size, \n","                 attention, predictor, generator, num_of_fields=10,\n","                 vocab_size=98, num_layers=1):\n","        super(HearthstoneDecoder, self).__init__()\n","        self.trg_embed = nn.Embedding(vocab_size, embedding_size) # embed_vec for each char\n","        \n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.rnn = nn.LSTM(enc_output_size + embedding_size, hidden_size, num_layers,\n","                           batch_first=True)  # z_t, h_(t-1) -> h_t\n","        self.attention = attention  # HearthstoneAttention\n","        self.predictor = predictor # \n","        self.generator = generator\n","        self.num_of_fields = num_of_fields\n","        self.hidden2pred = nn.Linear(hidden_size, num_of_fields + 1) # map hidden vectors to num. of predictors size vec\n","    def forward_step(self, prev_embed, hidden, all_enc_fields, mode=\"train\"):\n","\n","        attention_vec = self.attention(all_enc_fields, hidden[0]).view(1,1,-1) # (1,1,enc_output_size)\n","        prev_embed_attention = torch.cat([attention_vec, prev_embed], dim=2) \n","        print(prev_embed_attention.size())\n","        _, hidden = self.rnn(prev_embed_attention, hidden)\n","        pred_vec = self.hidden2pred(hidden[0])\n","        pred_probs = F.log_softmax(pred_vec)\n","        \n","        # assuming predictors return the length of the output embed vector seq\n","\n","        return pred_probs, attention_vec, hidden\n","\n","    def forward(self, trg_seq, all_enc_fields, input_card, hidden=None, max_output_len=None, \n","                mode=\"train\"):\n","\n","        if max_output_len is None:\n","            max_output_len = trg_seq.size(-1)\n","        \n","        if hidden is None:\n","            hidden = (torch.zeros((1,1,self.hidden_size)).to(device), \n","                      torch.zeros((1,1,self.hidden_size)).to(device))\n","        \n","        trg_emb = self.trg_embed(trg_seq)\n","\n","        predictor_selection = []\n","        attentions = []\n","        hiddens = [hidden[0]]\n","        outputs = []\n","        if mode == \"train\":    \n","            i = 0\n","            while i < max_output_len:\n","                prev_embed = trg_emb[:,i].unsqueeze(1)\n","                pred_probs, attention_vec, hidden = self.forward_step(prev_embed, hidden, all_enc_fields)\n","                pred_id = torch.argmax(pred_probs).item()\n","                #predictor = self.predictors(pred_id)\n","                if pred_id == self.num_of_fields:\n","                    output, seq_len = self.generator(hidden[0]), 1\n","                else:\n","                    output = self.predictor(pred_id, all_enc_fields, input_card) #, attention_vec, prev_hidden)\n","                    seq_len = len(output) \n","                i = i + seq_len\n","                if type(output) is list:\n","                    output = F.one_hot(torch.tensor([output]), num_classes=97).float()\n","                outputs.append(output)\n","        else:\n","            for i in range(max_output_len):\n","                prev_embed = trg_emb[:,i].unsqueeze(1)\n","                pred_probs, attention_vec, hidden = self.forward_step(prev_embed, hidden, all_enc_fields)    \n","                predictor_selection.append(pred_probs)\n","                attentions.append(attentions)\n","                hiddens.append(hidden)\n","        \n","        return predictor_selection, attentions, hiddens, torch.cat(outputs, dim=0).to(device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kXuipDlH7phc"},"source":["class HearthstoneEncoderDecoder(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        \"\"\"\n","        Inputs:\n","          - `encoder`: an `Encoder` object.\n","          - `decoder`: a `Decoder` object.\n","          - `generator`: a `Generator` object. Essentially a linear mapping. See\n","              the next code cell.\n","        \"\"\"\n","        super(HearthstoneEncoderDecoder, self).__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, card_spec_tokenized, input_card, trg_ids):\n","        \"\"\"Take in and process masked source and target sequences.\n","\n","        Inputs:\n","          `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n","            a batch of source sentences of word ids.\n","          `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n","            a batch of target sentences of word ids.\n","          `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n","            sequence length of `src_ids`.\n","\n","        Returns the decoder outputs, see the above cell.\n","        \"\"\"\n","        all_encoded_fields = self.encode(card_spec_tokenized)\n","        _ , _ , _ , outputs = self.decode(trg_ids, all_encoded_fields, input_card)\n","        return outputs\n","\n","    def encode(self, card_spec_tokenized):\n","        return self.encoder(card_spec_tokenized)\n","\n","    def decode(self, trg_ids, all_encoded_fields, input_card):\n","        return self.decoder(trg_ids, all_encoded_fields, input_card)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ci2CaFPmBV-H"},"source":["class HearthstonePredictor(nn.Module):\n","    def __init__(self, encoder_output_size, dataset):\n","        super(HearthstonePredictor, self).__init__()\n","        self.max_seq_len = max([len(card_spec.description.split(\" \")) for card_spec in training_dataset.hearthstone_card_specs])\n","        self.proj = nn.Linear(encoder_output_size, self.max_seq_len)\n","        self.input_size = encoder_output_size\n","        self.softmax = nn.Softmax(dim = 0)\n","    \n","    def forward(self, idx, all_encoded_fields, input_card):\n","        input_seq = all_encoded_fields[idx]\n","        word_seq = None\n","        if idx == 0:\n","            word_seq = input_card.name.split(\" \")\n","        elif idx == 1:\n","            word_seq = input_card.attack.split(\" \")\n","        elif idx == 2:\n","            word_seq = input_card.defense.split(\" \")\n","        elif idx == 3:\n","            word_seq = input_card.cost.split(\" \")\n","        elif idx == 4:\n","            word_seq = input_card.durability.split(\" \")\n","        elif idx == 5:\n","            word_seq = input_card.card_type.split(\" \")\n","        elif idx == 6:\n","            word_seq = input_card.player_cls.split(\" \")\n","        elif idx == 7:\n","            word_seq = input_card.race.split(\" \")\n","        elif idx == 8:\n","            word_seq = input_card.rarity.split(\" \")\n","        elif idx == 9:\n","            word_seq = input_card.description.split(\" \")\n","\n","        vocab_prob = self.softmax(self.proj(input_seq))\n","        word_id = torch.argmax(vocab_prob).item()\n","        id = word_id % len(word_seq)\n","        copy_seq = [char2id[char] for char in word_seq[id]]\n","        return copy_seq\n","\n","class HearthstoneGenerator(nn.Module):\n","    def __init__(self, enc_output_size, vocab_size=98):\n","        super(HearthstoneGenerator, self).__init__()\n","        self.proj = nn.Linear(enc_output_size, vocab_size, bias=False)\n","\n","    def forward(self, x):\n","        return F.log_softmax(self.proj(x), dim=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SijShSmB9Go7"},"source":["class HearthLossCompute:\n","    def __init__(self, criterion, opt=None):\n","        self.criterion = criterion\n","        self.opt = opt\n","\n","    def __call__(self, x, y, norm):\n","        \n","        loss = self.criterion(x.contiguous().view(-1, x.size(-1))[:y.size(-1),:],\n","                                y.contiguous().view(-1))\n","        loss = loss / norm\n","\n","        if self.opt is not None:    # training mode\n","            loss.backward()            \n","            self.opt.step()\n","            self.opt.zero_grad()\n","\n","        return loss.data.item() * norm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O3KTYArZ9wdd"},"source":["def run_epoch(size, data, model, loss_compute):\n","    \"\"\"Standard Training and Logging Function\"\"\"\n","    total_tokens = 0\n","    total_loss = 0\n","\n","    for i in tqdm(range(size)):\n","        card_spec_tok, input_card, trg_raw = data[i]\n","        trg_ids = [[char2id[ch] for ch in trg_raw[:-1]]]\n","        trg_ids = torch.tensor(trg_ids).to(device)\n","        outputs = model(card_spec_tok, input_card, trg_ids)\n","        loss = loss_compute(x=outputs, y=trg_ids,\n","                            norm=trg_ids.size(0))\n","        total_loss += loss\n","        total_tokens += (trg_ids != 97).data.sum().item()\n","\n","    print(f\"Total loss: {math.exp(total_loss / float(total_tokens))}\")\n","\n","    return math.exp(total_loss / float(total_tokens))\n","\n","def train(model, train_size, val_size, train_data, val_data, num_epochs, learning_rate=0.1):\n","    # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n","    # computing the loss.\n","    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=97)\n","    optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Keep track of dev ppl for each epoch.\n","    dev_ppls = []\n","\n","    for epoch in range(num_epochs):\n","        print(\"Epoch\", epoch)\n","\n","        model.train()\n","        train_ppl = run_epoch(train_size, train_data, model=model,\n","                                loss_compute=HearthLossCompute(criterion, optim))\n","\n","        model.eval()\n","        with torch.no_grad():        \n","            dev_ppl = run_epoch(val_size, val_data, model=model,\n","                                loss_compute=HearthLossCompute(criterion, None))\n","            print(\"Validation perplexity: %f\" % dev_ppl)\n","            dev_ppls.append(dev_ppl)\n","        \n","    return dev_ppls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7l2587G8GDl"},"source":["hearth_pred = HearthstonePredictor(300, training_dataset)\n","hearth_gen = HearthstoneGenerator(300)\n","hearth_decoder = HearthstoneDecoder(300, 300, 300, hearthstone_attention, \n","                                    hearth_pred, hearth_gen)\n","hearth_encoder_decoder = HearthstoneEncoderDecoder(hearthstone_encoder, hearth_decoder)\n","training_loader = data.DataLoader(training_dataset, batch_size=1, shuffle=True)\n","validation_loader = data.DataLoader(validation_dataset, batch_size=1, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"EpA-WCh28ZgI","outputId":"bf7390f2-b175-4fa0-9ba7-162e443c23a9"},"source":["epochs = 20\n","lr = 1e-3\n","hearth_encoder_decoder = hearth_encoder_decoder.to(device)\n","train(hearth_encoder_decoder, train_size, val_size, training_dataset, validation_dataset, epochs, lr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","  0%|          | 0/479 [00:00<?, ?it/s]\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  0%|          | 0/479 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n","torch.Size([1, 1, 600])\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-377-52671f446658>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhearth_encoder_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhearth_encoder_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhearth_encoder_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-375-76eab6ee2433>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_size, val_size, train_data, val_data, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         train_ppl = run_epoch(train_size, train_data, model=model,\n\u001b[0;32m---> 34\u001b[0;31m                                 loss_compute=HearthLossCompute(criterion, optim))\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-375-76eab6ee2433>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(size, data, model, loss_compute)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcard_spec_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_card\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         loss = loss_compute(x=outputs, y=trg_ids,\n\u001b[0;32m---> 12\u001b[0;31m                             norm=trg_ids.size(0))\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrg_ids\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m97\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-374-f7d411940073>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y, norm)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"]}]},{"cell_type":"code","metadata":{"id":"JRn_TNZWR0gC"},"source":[""],"execution_count":null,"outputs":[]}]}